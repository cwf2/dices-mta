{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235598d1",
   "metadata": {},
   "source": [
    "# 1. General notes\n",
    "\n",
    "### Local CTS server\n",
    "\n",
    "The public Scaife CTS server from Perseus doesn't provide Quintus. The text exists in the [canonical-greekLit](https://github.com/PerseusDL/canonical-greekLit) Git repo, but it's not configured for Nautilus to serve it. I created my own [canonical-greekLit fork](https://github.com/cwf2/canonical-greekLit) and edited Quintus until Nautilus was happy.\n",
    "\n",
    "This notebook should come with cached data, so you don't need to reprocess the texts. If you do want to replicate everything from scratch, then run the following code in a terminal window to install and run the CTS server locally.\n",
    "\n",
    "```bash\n",
    "    git clone https://github.com/cwf2/canonical-greekLit\n",
    "    capitains-nautilus canonical-greekLit --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c652b7",
   "metadata": {},
   "source": [
    "### Odyssey variant reading\n",
    "\n",
    "The DICES database has a speech by Circe to Odysseus beginning at Od. 10.456; but in the Perseus edition, 456 is missing and the speech begins at 457. I've manually changed the speech start line here to agree with Perseus, avoiding an error when we download the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789d6df",
   "metadata": {},
   "source": [
    "# 2. Steps for processing the speeches\n",
    "\n",
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2cc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicesapi import DicesAPI\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "from dicesapi.text import CtsAPI\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive, widgets\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c0fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.morphology.morphosyntax import Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3814024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grc-odycy-joint-trf==any\n",
      "  Downloading https://huggingface.co/chcaa/grc_odycy_joint_trf/resolve/main/grc_odycy_joint_trf-any-py3-none-any.whl (497.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.3/497.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy<3.6.0,>=3.5.0\n",
      "  Using cached spacy-3.5.4-cp310-cp310-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.9 in ./venv/lib/python3.10/site-packages (from grc-odycy-joint-trf==any) (1.1.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2.31.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (8.1.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (1.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (1.24.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (23.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (1.10.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (1.1.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (67.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./venv/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (2.0.1)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in ./venv/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (0.9.1)\n",
      "Requirement already satisfied: transformers<4.26.0,>=3.4.0 in ./venv/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (4.25.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (4.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (0.7.9)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (3.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (3.12.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (1.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in ./venv/lib/python3.10/site-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.10/site-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (2023.6.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->grc-odycy-joint-trf==any) (2.1.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (2023.10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->spacy-transformers<1.2.0,>=1.1.9->grc-odycy-joint-trf==any) (1.3.0)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.7.2\n",
      "    Uninstalling spacy-3.7.2:\n",
      "      Successfully uninstalled spacy-3.7.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "la-core-web-lg 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.5.4 which is incompatible.\n",
      "grc-proiel-lg 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.5.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed spacy-3.5.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install https://huggingface.co/chcaa/grc_odycy_joint_trf/resolve/main/grc_odycy_joint_trf-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925942a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b78ba62",
   "metadata": {},
   "source": [
    "## Initialize connections to DICES and CTS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafce442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to the database\n",
    "api = DicesAPI(dices_api='http://localhost:8000/api', logfile='dices.log')\n",
    "\n",
    "# initialize connection to digital libraries\n",
    "cts = CtsAPI(\n",
    "    dices_api = api,\n",
    "    servers = {\n",
    "        # None:  'https://scaife-cts.perseus.org/api/cts', # default\n",
    "        None: 'http://localhost:5000/cts', # use local server\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b29edc",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8beab634",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = {}\n",
    "words = {}\n",
    "works = ['iliad', 'odyssey', 'posthomerica']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4a362",
   "metadata": {},
   "source": [
    "### Download the speech metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b1dba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving speeches for iliad\n",
      "retrieved 698 results\n",
      "Retrieving speeches for odyssey\n",
      "retrieved 673 results\n",
      "Retrieving speeches for posthomerica\n",
      "retrieved 175 results\n"
     ]
    }
   ],
   "source": [
    "for work in works:\n",
    "    print(f'Retrieving speeches for {work}')\n",
    "            \n",
    "    speeches[work] = api.getSpeeches(work_title=work.title())\n",
    "    print('retrieved', len(speeches[work]), 'results')\n",
    "\n",
    "    # cludge for textual variant in Odyssey\n",
    "    if work.title() == 'Odyssey':\n",
    "        for s in speeches[work]:\n",
    "            if s.l_fi == '10.456':\n",
    "                s.l_fi = '10.457'\n",
    "    \n",
    "    # another cludge to remove the apologia\n",
    "    if work.title() == 'Odyssey':\n",
    "        speeches[work] = [s for s in speeches[work] if s.l_fi.split('.')[0] == s.l_la.split('.')[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aff097",
   "metadata": {},
   "source": [
    "### Download the text of the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a850bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c741523173194224a21c757e34427696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=698), Label(value='iliad: 0/698')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028c5cef975b428084ae47977eee95dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=671), Label(value='odyssey: 0/671')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6e0ff5a35e4ebd9f6c6ba80389bf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=175), Label(value='posthomerica: 0/175')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for work in works:\n",
    "\n",
    "    pbar = NotebookPBar(max=len(speeches[work]), prefix=f'{work}: ')\n",
    "\n",
    "    for s in speeches[work]:\n",
    "        if not hasattr(s, 'passage') or s.passage is None:\n",
    "            s.passage = cts.getPassage(s)\n",
    "        if s.passage is None:\n",
    "            print(f'Text download failed: {s}')\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5ca3b",
   "metadata": {},
   "source": [
    "### Parse the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ee48af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1527b8c3397a41c0b45372d18b5a3087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=698), Label(value='iliad: 0/698')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/Documents/git/dices-mta/venv/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'la_core_web_sm' (3.5.2) was trained with spaCy v3.5.2 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/chris/Documents/git/dices-mta/venv/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'grc_proiel_sm' (3.5.3) was trained with spaCy v3.5.3 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e305062c288a4b7e89b119aca4faaca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=671), Label(value='odyssey: 0/671')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620d37a10ff447ae9514009cacc9b616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='info', max=175), Label(value='posthomerica: 0/175')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for work in works:\n",
    "    \n",
    "    pbar = NotebookPBar(max=len(speeches[work]), prefix=f'{work}: ')\n",
    "\n",
    "    for s in speeches[work]:\n",
    "        pbar.update()\n",
    "        if not hasattr(s, 'passage') or s.passage is None:\n",
    "            print('no passage:', s)\n",
    "            continue\n",
    "        s.passage.runCltkPipeline()\n",
    "        s.passage.runSpacyPipeline()\n",
    "        if s.passage.cltk_doc is None:\n",
    "            print(f'CLTK failed: {s}')\n",
    "        if s.passage.spacy_doc is None:\n",
    "            print(f'SpaCy failed: {s}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5f876",
   "metadata": {},
   "source": [
    "### Format tokens as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37abec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTokenTable(speeches):\n",
    "    '''Create a DataFrame with one row per token'''\n",
    "    \n",
    "    speeches = [s for s in speeches if s.passage.nlp is not None]\n",
    "    \n",
    "    words = pd.DataFrame(dict(\n",
    "        speech_id = s.id,\n",
    "        book = s.l_fi.split('.')[0],\n",
    "        line = s.passage.line_array[s.passage.getLineIndex(w)]['n'],\n",
    "        l_ind = s.passage.getLineIndex(w)+1,\n",
    "        spkr = s.getSpkrString(),\n",
    "        addr = s.getAddrString(),\n",
    "        gend_spkr = ';'.join(sorted(set(inst.gender for inst in s.spkr))),\n",
    "        gend_addr = ';'.join(sorted(set(inst.gender for inst in s.addr))),\n",
    "        being_spkr = ';'.join(sorted(set(inst.being for inst in s.spkr))),\n",
    "        being_addr = ';'.join(sorted(set(inst.being for inst in s.addr))),\n",
    "        disg = ';'.join([spkr.disg for spkr in s.spkr if spkr.disg is not None]),\n",
    "        type = ';'.join([t['type'] for t in s._attributes['tags']]),\n",
    "        token = w.string,\n",
    "        lemma = w.lemma,\n",
    "        upos = w.upos,\n",
    "        case = w.features[Case][0].name if Case in w.features.keys() else None,\n",
    "        is_voc = 'vocative' in str(w.features),\n",
    "        features = str(w.features),\n",
    "    ) for s in speeches for w in s.passage.cltk_doc)\n",
    "\n",
    "    # filter out punctuation tokens\n",
    "    words = words[(words.token != '.') & (words.upos != 'PUNCT')]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ee8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSpacyTable(speeches):\n",
    "    '''Create a DataFrame with one row per token'''\n",
    "    \n",
    "    speeches = [s for s in speeches if s.passage.spacy_doc is not None]\n",
    "    \n",
    "    words = pd.DataFrame(dict(\n",
    "        speech_id = s.id,\n",
    "        book = s.l_fi.split('.')[0],\n",
    "        line = s.passage.line_array[s.passage.getLineIndex(w)]['n'],\n",
    "        l_ind = s.passage.getLineIndex(w)+1,\n",
    "        spkr = s.getSpkrString(),\n",
    "        addr = s.getAddrString(),\n",
    "        gend_spkr = ';'.join(sorted(set(inst.gender for inst in s.spkr))),\n",
    "        gend_addr = ';'.join(sorted(set(inst.gender for inst in s.addr))),\n",
    "        being_spkr = ';'.join(sorted(set(inst.being for inst in s.spkr))),\n",
    "        being_addr = ';'.join(sorted(set(inst.being for inst in s.addr))),\n",
    "        disg = ';'.join([spkr.disg for spkr in s.spkr if spkr.disg is not None]),\n",
    "        type = ';'.join([t['type'] for t in s._attributes['tags']]),\n",
    "        token = w.text,\n",
    "        lemma = w.lemma_,\n",
    "        upos = w.pos_,\n",
    "        case = w.morph.to_dict().get('Case'),\n",
    "        is_voc = w.morph.to_dict().get('Case') == 'Voc',\n",
    "        features = str(w.morph),\n",
    "    ) for s in speeches for w in s.passage.spacy_doc)\n",
    "\n",
    "    # filter out punctuation tokens\n",
    "    words = words[(words.token != '.') & (words.upos != 'PUNCT')]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cba730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk = {}\n",
    "spacy = {}\n",
    "for work in works:\n",
    "    cltk[work] = makeTokenTable(speeches[work])\n",
    "    spacy[work] = makeSpacyTable(speeches[work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14784dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk['iliad'].to_csv('iliad_cltk.csv', index=False)\n",
    "spacy['iliad'].to_csv('iliad_spacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec746b",
   "metadata": {},
   "source": [
    "## 3. Run the whole workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for work in works:\n",
    "    print('Processing', work.title())\n",
    "    \n",
    "    # local file paths\n",
    "    cache = os.path.join('data', f'{work}_speeches.pickle')\n",
    "    output = os.path.join('data', f'{work}.csv')\n",
    "    \n",
    "    # use cached data if present\n",
    "    if os.path.exists(cache):\n",
    "        with open(cache, 'rb') as f:\n",
    "            speeches[work] = pickle.load(f)\n",
    "        print('loaded', len(speeches[work]), 'cached results')\n",
    "    else:    \n",
    "        speeches[work] = dlSpeechData(work)\n",
    "        dlSpeechText(speeches[work])\n",
    "        parseSpeechText(speeches[work])\n",
    "        with open(cache, 'wb') as f:\n",
    "            pickle.dump(speeches[work], f)\n",
    "        print('saved', len(speeches[work]), 'results to', cache)\n",
    "    \n",
    "    # generate tabular data\n",
    "    words[work] = makeTokenTable(speeches[work])\n",
    "    \n",
    "    # save output\n",
    "    print(f'Writing {output}')\n",
    "    words[work].to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae26c69",
   "metadata": {},
   "source": [
    "### inspect the table of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431139",
   "metadata": {},
   "outputs": [],
   "source": [
    "words['iliad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3b2d3",
   "metadata": {},
   "source": [
    "# 4. summary statistics\n",
    "\n",
    "## helper functions\n",
    "\n",
    "### Simple count of vocatives by book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableByBook(work):\n",
    "    df = words[work][words[work].is_voc].pivot_table(\n",
    "        index = 'book',\n",
    "        values = 'speech_id',\n",
    "        aggfunc = 'count',\n",
    "        sort = False,\n",
    "        fill_value = 0,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def getPlotByBook(work):\n",
    "    df = tableByBook(work)\n",
    "    plot = df.plot.bar(\n",
    "        title = f'Vocatives in the {work.title()}',\n",
    "        legend = False,\n",
    "        rot = False,\n",
    "        ylabel = 'count',\n",
    "        figsize = (10,4),\n",
    "    )\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235cc99",
   "metadata": {},
   "source": [
    "### Normalized for book length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b95867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableByBookNorm(work):\n",
    "\n",
    "    df = words[work].pivot_table(\n",
    "        index = 'book',\n",
    "        values = 'speech_id',\n",
    "        columns = 'is_voc',\n",
    "        aggfunc = 'count',\n",
    "        sort = False,\n",
    "        fill_value = 0,\n",
    "    ).rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "    df['prop'] = df['voc'] / (df['voc'] + df['other']) * 1000\n",
    "\n",
    "    return df\n",
    "\n",
    "def getPlotByBookNorm(work):\n",
    "    df = getTableByBookNorm(work)\n",
    "    plot = df['prop'].plot.bar(\n",
    "        title = f'Vocatives in the {work.title()}',\n",
    "        legend = False,\n",
    "        ylabel = 'count per 1000 words',\n",
    "        rot = False,\n",
    "        figsize = (10,4),\n",
    "    )\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1b6ce",
   "metadata": {},
   "source": [
    "### by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableBySpeaker(work):\n",
    "    \n",
    "    df = words[work].pivot_table(\n",
    "        index = 'spkr',\n",
    "        values = 'speech_id',\n",
    "        columns = 'is_voc',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "    df['prop'] = round(df['voc'] / (df['voc'] + df['other']) * 1000, 2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38189a14",
   "metadata": {},
   "source": [
    "### by part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415252f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableByPOS(work):\n",
    "\n",
    "    df = words[work].pivot_table(\n",
    "        index = 'upos',\n",
    "        values = 'speech_id',\n",
    "        columns = 'is_voc',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    ).rename(columns={True:'voc', False:'other'})\n",
    "\n",
    "    df.sort_values('voc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e10a8",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "### Normalized vocatives by book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(interactive(getPlotByBookNorm, work=works))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815de963",
   "metadata": {},
   "source": [
    "### By Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = lambda work: display(getTableBySpeaker(work))\n",
    "interactive(view, work=works)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f429a",
   "metadata": {},
   "source": [
    "#### greatest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = lambda work, n: display(getTableBySpeaker(work).sort_values('voc', ascending=False)[:n])\n",
    "display(interactive(view, work=works, n=widgets.IntSlider(min=10, max=100, step=5, value=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683ba9f",
   "metadata": {},
   "source": [
    "#### highest proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d884ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = lambda work, n: display(getTableBySpeaker(work).sort_values('prop', ascending=False)[:n])\n",
    "display(interactive(view, work=works, n=widgets.IntSlider(min=10, max=100, step=5, value=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf33575",
   "metadata": {},
   "source": [
    "#### highest proportion among speakers of at least 1000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(work, min_words, n_results):\n",
    "    df = getTableBySpeaker(work)\n",
    "    df = df[(df.other + df.voc) >= min_words]\n",
    "    df = df.sort_values('prop', ascending=False)[:n_results]\n",
    "    display(df)\n",
    "\n",
    "display(interactive(view, work=works, n_results=widgets.IntSlider(min=10, max=100, step=5, value=10),\n",
    "                   min_words = widgets.IntSlider(min=100, max=2000, step=100, value=1000)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
