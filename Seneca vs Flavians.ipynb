{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4623ae9a",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0cbfd67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from cltk import NLP\n",
    "from dicesapi import DicesAPI\n",
    "from dicesapi.text import CtsAPI, Passage\n",
    "from dicesapi.jupyter import NotebookPBar\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import f_oneway, tukey_hsd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# custom code for this notebook\n",
    "#   - see seneca_experiment.py\n",
    "from seneca_experiment import SenecaSpeech, getTags, tagtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c411d",
   "metadata": {},
   "source": [
    "### Connections to remote databases, local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304de466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Seneca text\n",
    "seneca_text_file = os.path.join('data', 'seneca_speeches.txt')\n",
    "\n",
    "# remote endpoints\n",
    "api = DicesAPI(logfile='dices.log')\n",
    "cts = CtsAPI(dices_api=api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fdf18b",
   "metadata": {},
   "source": [
    "In addition to the hand-curated input from Bernhardt for Seneca's speeches, I'm using local csv files to store the NLP annotations for the tokenized speeches, since parsing all that text takes some time. \n",
    "\n",
    "**NB**: Delete the csv files to rerun everything from scratch. This might run prohibitively slowly on binder, but it only takes half an hour or so on my old laptop. You could try just deleting the Seneca cache to see the parsing in operation on a smaller speech set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289fe14d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cache files\n",
    "cache_seneca = os.path.join('data', 'seneca_tokens.csv')\n",
    "cache_flavians = os.path.join('data', 'flavian_tokens.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f549d82",
   "metadata": {},
   "source": [
    "# Part 1: Seneca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa79d0",
   "metadata": {},
   "source": [
    "### Use cache if it's present\n",
    "\n",
    "If the CSV files with all the parsed tokens is here, we just use that and skip the next several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca8aaf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5599 records from data/seneca_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(cache_seneca):\n",
    "    sen_tokens = pd.read_csv(cache_seneca)\n",
    "    print(f'Loaded {len(sen_tokens)} records from {cache_seneca}')\n",
    "    SKIP_SENECA = True\n",
    "else:\n",
    "    SKIP_SENECA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba3f5b",
   "metadata": {},
   "source": [
    "### Read Bernhardt's text and parse into speech-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2e856c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_SENECA:\n",
    "    with open(seneca_text_file) as f:\n",
    "        text = f.read()\n",
    "        if text:\n",
    "            sen_texts = re.split('\\n\\n+', text)\n",
    "\n",
    "    # how many speeches did we get?\n",
    "    print(f'Loaded {len(sen_texts)} speeches from {seneca_text_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9662ba20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_SENECA:\n",
    "    sen_speeches = [SenecaSpeech(id=i, text=s) for i, s in enumerate(sen_texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4e663",
   "metadata": {},
   "source": [
    "### Run CLTK's NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17912b72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_SENECA:\n",
    "    for s in sen_speeches:\n",
    "        print (s, '...', end='')\n",
    "\n",
    "        if (not hasattr(s.passage, 'cltk')) or s.passage.cltk is None:\n",
    "            s.passage.runCltkPipeline(remove_punct=True)\n",
    "        if s.passage.cltk is not None:\n",
    "            print('OK')\n",
    "        else:\n",
    "            print('fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea450b20",
   "metadata": {},
   "source": [
    "### Convert to tabular form\n",
    "\n",
    "`sen_tokens` is a table with one row per token. It includes basic details on the passage as well as universal part of speech label and lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272c1c4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 5599 records to data/seneca_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_SENECA:\n",
    "    sen_tokens = pd.DataFrame(dict(\n",
    "        id = s.id,\n",
    "        auth = 'Seneca',\n",
    "        tags = 'trag',\n",
    "        l_fi = s.l_fi,\n",
    "        l_la = s.l_la,\n",
    "        spkr = s.spkr,\n",
    "        lem = w.lemma,\n",
    "        pos = w.upos,\n",
    "    ) for s in sen_speeches for w in s.passage.cltk)\n",
    "    \n",
    "sen_tokens.to_csv(cache_seneca)\n",
    "print(f'Writing {len(sen_tokens)} records to {cache_seneca}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a44254",
   "metadata": {},
   "source": [
    "### Example of tabular data\n",
    "\n",
    "`sen_tokens` is a table with one row per token. It includes basic details on the passage as well as universal part of speech label and lemma. This is what is stored in the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6b3122",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>auth</th>\n",
       "      <th>tags</th>\n",
       "      <th>l_fi</th>\n",
       "      <th>l_la</th>\n",
       "      <th>spkr</th>\n",
       "      <th>lem</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>Iuno</td>\n",
       "      <td>Soror</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>Iuno</td>\n",
       "      <td>Tonantis</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>Iuno</td>\n",
       "      <td>(</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>Iuno</td>\n",
       "      <td>hic</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1</td>\n",
       "      <td>278</td>\n",
       "      <td>Iuno</td>\n",
       "      <td>enim</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>5594</td>\n",
       "      <td>5594</td>\n",
       "      <td>52</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1341</td>\n",
       "      <td>1344</td>\n",
       "      <td>Theseus</td>\n",
       "      <td>innocens</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>5595</td>\n",
       "      <td>5595</td>\n",
       "      <td>52</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1341</td>\n",
       "      <td>1344</td>\n",
       "      <td>Theseus</td>\n",
       "      <td>terra</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>5596</td>\n",
       "      <td>5596</td>\n",
       "      <td>52</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1341</td>\n",
       "      <td>1344</td>\n",
       "      <td>Theseus</td>\n",
       "      <td>qui</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5597</th>\n",
       "      <td>5597</td>\n",
       "      <td>5597</td>\n",
       "      <td>52</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1341</td>\n",
       "      <td>1344</td>\n",
       "      <td>Theseus</td>\n",
       "      <td>super</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5598</th>\n",
       "      <td>5598</td>\n",
       "      <td>5598</td>\n",
       "      <td>52</td>\n",
       "      <td>Seneca</td>\n",
       "      <td>trag</td>\n",
       "      <td>1341</td>\n",
       "      <td>1344</td>\n",
       "      <td>Theseus</td>\n",
       "      <td>soleo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5599 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0  id    auth  tags  l_fi  l_la     spkr   \n",
       "0                0           0   0  Seneca  trag     1   278     Iuno  \\\n",
       "1                1           1   0  Seneca  trag     1   278     Iuno   \n",
       "2                2           2   0  Seneca  trag     1   278     Iuno   \n",
       "3                3           3   0  Seneca  trag     1   278     Iuno   \n",
       "4                4           4   0  Seneca  trag     1   278     Iuno   \n",
       "...            ...         ...  ..     ...   ...   ...   ...      ...   \n",
       "5594          5594        5594  52  Seneca  trag  1341  1344  Theseus   \n",
       "5595          5595        5595  52  Seneca  trag  1341  1344  Theseus   \n",
       "5596          5596        5596  52  Seneca  trag  1341  1344  Theseus   \n",
       "5597          5597        5597  52  Seneca  trag  1341  1344  Theseus   \n",
       "5598          5598        5598  52  Seneca  trag  1341  1344  Theseus   \n",
       "\n",
       "           lem    pos  \n",
       "0        Soror   NOUN  \n",
       "1     Tonantis   VERB  \n",
       "2            (  PUNCT  \n",
       "3          hic   PRON  \n",
       "4         enim    ADV  \n",
       "...        ...    ...  \n",
       "5594  innocens    ADJ  \n",
       "5595     terra   NOUN  \n",
       "5596       qui   PRON  \n",
       "5597     super   NOUN  \n",
       "5598     soleo   VERB  \n",
       "\n",
       "[5599 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c32d6",
   "metadata": {},
   "source": [
    "### Shape of the data\n",
    "\n",
    "#### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c99e5fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sen_speeches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43msen_speeches\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m speeches, totalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sen_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sen_speeches' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(sen_speeches)} speeches, totalling {len(sen_tokens)} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392f359",
   "metadata": {},
   "source": [
    "#### Speech length distribution\n",
    "\n",
    "How many long speeches? How many short speeches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c522449",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sen_tokens.groupby('id').size().hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c0f4b4",
   "metadata": {},
   "source": [
    "One outlier is making it hard to see fine details---let's zoom in on the left part of the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344bf91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sen_tokens.groupby('id').size().hist(range=[0,400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f9a78",
   "metadata": {},
   "source": [
    "Most of these speeches are pretty short... about 100 words or fewer.\n",
    "\n",
    "### Part of speech counts\n",
    "\n",
    "How much does Seneca use each of the parts of speech (according to CLTK's classification)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b51d1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = sen_tokens.groupby('pos').size().plot.bar()\n",
    "ax.set_xlabel('part of speech')\n",
    "ax.set_ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17091f",
   "metadata": {},
   "source": [
    "**Two important notes:**\n",
    "\n",
    "1. CLTK's part of speech tags don't include interjections here! I don't know why not... I'm fairly confident that I've seen results on Greek texts that do include interjections. But for whatever reason, any words that we consider interjections in this text are being labelled as other parts of speech.\n",
    "\n",
    "2. Even though I tried to filter out punctuation before parsing (it generally improves the lemmatization), CLTK is labelling some tokens as `PUNCT`. These seem mostly to be actual punctuation marks that my initial efforts missed.\n",
    "\n",
    "### Tabulate POS counts per speech\n",
    "\n",
    "This table tallies part of speech tags by speech. Each row is one speech. Speeches with more words will have higher counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f1bb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sen_pos = pd.crosstab(sen_tokens.id, sen_tokens.pos)\n",
    "sen_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a491057",
   "metadata": {},
   "source": [
    "### Same, but normalized for speech length\n",
    "\n",
    "This is the same table, but we divide each row by the total number of tokens, making long speeches and short speeches more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7ebec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sen_norm = pd.crosstab(sen_tokens.id, sen_tokens.pos, normalize='index')\n",
    "sen_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c63a6",
   "metadata": {},
   "source": [
    "### Comparing speeches\n",
    "\n",
    "Does part of speech use tell us anything interesting about the text? Let's try a simple comparison: how often does each character use adjectives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b87f18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sen_labels = sen_tokens.groupby('id').agg({'auth':'first', 'spkr':'first', 'tags':'first'})\n",
    "pd.concat([sen_norm, sen_labels], axis=1).boxplot(column='ADJ', by='spkr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952ac93",
   "metadata": {},
   "source": [
    "There's a lot of overlap, but it looks like Theseus uses adjectives more than Hercules, for example. Let's look at the two distributions in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df38c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(sen_norm.loc[sen_labels.spkr=='Hercules']['ADJ'], alpha=0.5, label='Hercules')\n",
    "ax.hist(sen_norm.loc[sen_labels.spkr=='Theseus']['ADJ'], alpha=0.5, label='Theseus')\n",
    "ax.legend()\n",
    "ax.set_title('Character use of adjectives')\n",
    "ax.set_xlabel('ADJ / all tokens')\n",
    "ax.set_ylabel('speeches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a261e3",
   "metadata": {},
   "source": [
    "The two peaks are definitely different, but we also see how patchy the data is, particularly for Theseus.\n",
    "At the end of the day, there aren't a lot of speeches here, but this is at least a sign that we might look profitably look more closely at Theseus' use of adjectives.\n",
    "\n",
    "**But on the other hand...**\n",
    "\n",
    "Hercules uses more subordinating conjunctions than Theseus. So it's not just that he's a less-sophisticated speaker..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373e7bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([sen_norm, sen_labels], axis=1).boxplot(column='SCONJ', by='spkr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e71b7",
   "metadata": {},
   "source": [
    "# Part 2: Flavians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296541d",
   "metadata": {},
   "source": [
    "### Use cache if it's present\n",
    "\n",
    "If the CSV files with all the parsed tokens is here, we just use that and skip the next several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822da88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(cache_flavians):\n",
    "    flav_tokens = pd.read_csv(cache_flavians)\n",
    "    print(f'Loaded {len(flav_tokens)} records from {cache_flavians}')\n",
    "    SKIP_FLAV = True\n",
    "else:\n",
    "    SKIP_FLAV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b0136",
   "metadata": {},
   "source": [
    "### Retrieve the speeches from DICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f82b82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_FLAV:\n",
    "    flav_speeches = sorted(\n",
    "                    api.getSpeeches(author_name='Statius') + \\\n",
    "                    api.getSpeeches(author_name='Silius') + \\\n",
    "                    api.getSpeeches(author_name='Valerius Flaccus'))\n",
    "    print(f'Retrieved {len(flav_speeches)} speeches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576ca22",
   "metadata": {},
   "source": [
    "### Retrieve text from Perseus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac97468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_FLAV:\n",
    "    pbar = NotebookPBar(max=len(flav_speeches))\n",
    "\n",
    "    for i, s in enumerate(flav_speeches):\n",
    "        if (not hasattr(s, 'passage')) or s.passage == None:\n",
    "            s.passage = cts.getPassage(s)\n",
    "            if s.passage is None:\n",
    "                print(f'failed: {s}')\n",
    "        pbar.update(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e87b8",
   "metadata": {},
   "source": [
    "### Parse with CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2522eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_FLAV:\n",
    "    pbar = NotebookPBar(max=len(flav_speeches))\n",
    "\n",
    "    for i, s in enumerate(flav_speeches):\n",
    "        if s.passage is not None:\n",
    "            if (not hasattr(s.passage, 'cltk')) or s.passage.cltk == None:\n",
    "                s.passage.runCltkPipeline(remove_punct=True)\n",
    "        pbar.update(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db457dff",
   "metadata": {},
   "source": [
    "### Generate a bit table of tokens\n",
    "\n",
    "As with `sen_tokens` above, this has one row per token.\n",
    "\n",
    "**Note:** Both the `spkr` and `tags` columns can theoretically contain multiple values, since it's pretty common for speeches to be tagged with multiple speech types, and on rare occasions a single speech has two speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cd4a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_FLAV:\n",
    "    flav_tokens = []\n",
    "    for s in flav_speeches:\n",
    "        if s.passage is not None and s.passage.cltk is not None:\n",
    "            for w in s.passage.cltk:\n",
    "                flav_tokens.append(dict(\n",
    "                    id = s.id,\n",
    "                    auth = s.author.name,\n",
    "                    work = s.work.title,\n",
    "                    l_fi = s.l_fi,\n",
    "                    l_la = s.l_la,\n",
    "                    tags = getTags(s),\n",
    "                    spkr = [spkr.name for spkr in s.spkr],\n",
    "                    lem = w.lemma,\n",
    "                    pos = w.upos,\n",
    "                ))\n",
    "    flav_tokens = pd.DataFrame(flav_tokens)\n",
    "    \n",
    "flav_tokens.to_csv(cache_flavians)\n",
    "print(f'Writing {len(flav_tokens)} records to {cache_flavians}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60d29a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flav_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47d88a",
   "metadata": {},
   "source": [
    "### Tag counts\n",
    "\n",
    "#### How many tokens are there for each tag type?\n",
    "\n",
    "Note that any time we're comparing between speech types, we need to break down the multiple values in the `tags` column. That means creating multiple copies of these rows, one for each tag. This is done with the pandas `explode` method.\n",
    "\n",
    "Note that in tables comparing types like this, the column totals aren't reflective of true number of speeches/tokens in the corpus, since some speeches are considered more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e3b3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tag_count = flav_tokens[['id','tags']].explode('tags').groupby('tags').agg(\n",
    "    tokens = pd.NamedAgg(column='id', aggfunc='count'),\n",
    "    speeches = pd.NamedAgg(column='id', aggfunc='nunique'),\n",
    ").sort_values(by='speeches', ascending=False)\n",
    "tag_count['label'] = [tagtype[t] for t in tag_count.index]\n",
    "tag_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e9d97",
   "metadata": {},
   "source": [
    "**Distribution of speech lengths across types**\n",
    "\n",
    "I know that some speeches are much longer than others. Is there a significant difference in speech length across the speech type tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8e426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "\n",
    "for s in flav_speeches:\n",
    "    if s.passage is not None and s.passage.cltk is not None:\n",
    "        x.append(dict(\n",
    "            length = len([w for w in s.passage.cltk]),\n",
    "            tags = getTags(s),\n",
    "        ))\n",
    "x = pd.DataFrame([row for row in x if row['length'] < 500])\n",
    "x = x.explode('tags')\n",
    "\n",
    "x.boxplot(by='tags', figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b0e6c",
   "metadata": {},
   "source": [
    "It's not a surprise that, on average, narrative speeches are the longest.\n",
    "\n",
    "\n",
    "### Calculate POS feature vectors for speeches\n",
    "\n",
    "As we did above for Seneca, here we tally part of speech counts for all the Flavians' speeches.\n",
    "\n",
    "#### Raw counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992afc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flav_pos = pd.crosstab(flav_tokens.id, flav_tokens.pos)\n",
    "flav_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef9688",
   "metadata": {},
   "source": [
    "#### Normalized by speech\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44887c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flav_norm = pd.crosstab(flav_tokens.id, flav_tokens.pos, normalize='index')\n",
    "flav_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f16fe",
   "metadata": {},
   "source": [
    "### Distribution of POS tags by speech type\n",
    "\n",
    "Here we examine visually whether the proportion of different parts of speech varies much between speech types.\n",
    "\n",
    "First, collect the speech metadata we might use to compare speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1193e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flav_labels = flav_tokens.groupby('id').agg({'auth':'first', 'spkr':'first', 'tags':'first'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732f8f0",
   "metadata": {},
   "source": [
    "#### Build a table with one row per tag\n",
    "\n",
    "We start with the normalized POS features above, but speeches with multiple types are broken out into multiple (duplicate) rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d43bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = flav_norm.copy()\n",
    "x['tags'] = flav_labels.tags\n",
    "x = x.explode('tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c93880",
   "metadata": {},
   "source": [
    "#### Draw some box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e160107",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feat in ['VERB', 'NOUN', 'ADJ']:\n",
    "    x[[feat, 'tags']].boxplot(by='tags', figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecbf180",
   "metadata": {},
   "source": [
    "# Part 3: Comparison\n",
    "\n",
    "First, join the Seneca and Flavians tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7456de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_tokens = pd.concat([sen_tokens, flav_tokens], ignore_index=True)\n",
    "all_labels = pd.concat([sen_labels, flav_labels])\n",
    "all_pos = pd.concat([sen_norm, flav_norm], ignore_index=True).set_index(all_labels.index)\n",
    "all_pos = all_pos.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611f203",
   "metadata": {},
   "source": [
    "### Calculate PCA features\n",
    "\n",
    "Instead of using individual POS tags, we can create a more holistic featureset using principal components analysis. Each of the resulting features incorporates elements of all the POS parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d722b11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=3)\n",
    "pca_features = pca_model.fit_transform(all_pos)\n",
    "all_pca = pd.DataFrame(\n",
    "    index=all_pos.index,\n",
    "    data=pca_features, \n",
    "    columns=['PC1', 'PC2', 'PC3'])\n",
    "all_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ee1a8",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Let's plot all the speeches according to the first two principal components, grouped by author. We might expect that if the authors have very different syntactic styles, we could see a noticeable difference between the clouds of coloured dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ab456",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "groups = all_pca.groupby(all_labels.auth.values)\n",
    "\n",
    "feat_x = 'PC1'\n",
    "feat_y = 'PC2'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for name, group in groups:\n",
    "    ax.plot(group[[feat_x]], group[[feat_y]], marker='o', linestyle='', ms=3, label=name)\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f649d15",
   "metadata": {},
   "source": [
    "\n",
    "For my part,  I don't think I see any interesting separation between the groups, which suggests that the primary stylistic difference between these speeches is elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95bddcc",
   "metadata": {},
   "source": [
    "### PCA features by type tag\n",
    "\n",
    "This series of plots shows one speech type at a time, according to the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9b841",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = pd.concat([all_pca, all_labels.tags], axis=1).explode('tags')\n",
    "\n",
    "groups = x.groupby('tags')\n",
    "\n",
    "ncols = 3\n",
    "nrows = math.ceil(len(groups)/ncols)\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(8,20), layout='constrained')\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for name, group in groups:\n",
    "    ax = axs[i, j]\n",
    "    ax.plot(group.PC1, group.PC2, marker='o', linestyle='', ms=2, label=name)\n",
    "    ax.set_xlim((-0.2, 0.2))\n",
    "    ax.set_ylim((-0.2, 0.2))\n",
    "    ax.set_title(tagtype[name][:18])\n",
    "    i = i + 1\n",
    "    if i >= nrows:\n",
    "        i = 0\n",
    "        j = j + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674fe37",
   "metadata": {},
   "source": [
    "It would have been cool if some types of speeches were more like Seneca than others, but I don't think I see a big difference here.\n",
    "\n",
    "The one thing that stands out visually is a separation between instructions and greetings. Let's take a closer look at that, just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373dd21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_feat = 'PC1'\n",
    "y_feat = 'PC2'\n",
    "names = ['pra', 'gre']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for name in names:\n",
    "    ax.plot(x.loc[x.tags==name][x_feat], x.loc[x.tags==name][y_feat], marker='o', linestyle='', label=tagtype[name])\n",
    "    ax.set_xlabel(x_feat)\n",
    "    ax.set_ylabel(y_feat)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f5f42",
   "metadata": {},
   "source": [
    "## Lemma-based features\n",
    "\n",
    "Putting aside POS tags, let's look at specific lemmata for our features.\n",
    "\n",
    "**Feature selection**\n",
    "\n",
    "We have the option of hand-selecting a feature set---that is, a bundle of lemmata that we care about. One trick is that word frequency declines exponentially, so the number of samples containing a given word declines very rapidly as we look at less frequent words. If the words we choose aren't in the samples, then they're not useful in measuring how the samples compare to one another.\n",
    "\n",
    "### Lemma counts\n",
    "\n",
    "Let's begin by creating a tally of how often each lemma occurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91249a3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4c052",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# omit punctuation\n",
    "mask = all_tokens.pos != 'PUNCT'\n",
    "\n",
    "lem_count = all_tokens[mask].groupby('lem').agg(\n",
    "    tokens = pd.NamedAgg(column='id', aggfunc='count'),\n",
    "    speeches = pd.NamedAgg(column='id', aggfunc='nunique'),\n",
    ").sort_values(by='tokens', ascending=False)\n",
    "lem_count[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bb865",
   "metadata": {},
   "source": [
    "### An example featureset \n",
    "\n",
    "I've hand-selected some of the most frequent words here. You can replace these with anything you want and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81235d65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    'et', 'qui', 'hic', 'tu', 'ego', 'sum', 'in', 'non', 'nec', 'atque', \n",
    "    'do', 'ille', 'noster', 'si', 'iam', 'ad', 'quis', 'nunc', 'tuus', \n",
    "    'ipse', 'sed', 'meus', 'fero', 'per', 'magnus', 'bellum', 'deus', \n",
    "    'cum', 'aut', 'manus', 'pater', 'o', 'nos', 'omnis', 'arma', 'sic',\n",
    "    'ab', 'ut', 'ago', 'nascor', 'dexter', 'sanguis', 'labor', 'terra', \n",
    "    'facio', 'eo', 'primus', 'aio', 'gens',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c8c2e",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "Let's extract the lemma frequencies for each speech, considering only these features.\n",
    "\n",
    "#### A table of feature vectors\n",
    "\n",
    "The resulting table has one row per speech, and one column for each of the lemmata in our featureset. The speech is  represented by *n* feature frequencies, so we can think of it as a point, or vector, in an *n*-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c941f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_vec = pd.crosstab(all_tokens.id, all_tokens.lem, normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba520252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = all_vec.loc[:, keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d0262",
   "metadata": {},
   "source": [
    "**Now add the tag data**\n",
    "\n",
    "In order to compare between speech types, we're going to add in a tag column. Then as we did above, we have to break out any rows with multiple tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638303bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x['tags'] = all_labels.tags\n",
    "x = x.explode('tags')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f0bb8",
   "metadata": {},
   "source": [
    "**Example: how does use of *sum* vary across speech types?**\n",
    "\n",
    "Answer: not much. But compare, for example, consolation (`con`) and challenge (`cha`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0abf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x.boxplot(column='sum', by='tags', figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf676e",
   "metadata": {},
   "source": [
    "### Log frequencies\n",
    "\n",
    "The distributions of lemmata are not normal across these samples, because of the exponential rate of decrease mentioned above. That's one reason why so many of the boxes in the plot above have a really low mean but then a bunch of outliers at the top. \n",
    "\n",
    "If we consider not the frequencies but the log of the frequencies, it's a little easier to see the variation. Any samples where a given word does not occur will have a frequency of `0`; in the log version we'll replace that with the placeholder value `NaN`, since the log of 0 can't be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c844391",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_log = x.copy()\n",
    "for col in x_log.columns:\n",
    "    if col != 'tags':\n",
    "        x_log[col] = x[col].apply(np.log).values\n",
    "x_log[x==0] = np.nan\n",
    "x_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1105fd8",
   "metadata": {},
   "source": [
    "### Comparing log lemma frequencies between speech types\n",
    "\n",
    "We already know that our main interest in terms of speech types is oracular speech. Let's see how it compares with other speech types using the new featureset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7303f",
   "metadata": {},
   "source": [
    "#### Visualizing with boxplots\n",
    "\n",
    "Here we compare a couple of our features across oracular speech, taunts and challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358ece7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['et', 'qui', 'sum']\n",
    "row_select = ['ora', 'tau', 'cha']\n",
    "\n",
    "for feat in features:\n",
    "    mask = x_log.tags.isin(row_select) \n",
    "    x_log[mask].boxplot(column=feat, by='tags', figsize=(4,2))\n",
    "    plt.suptitle('')\n",
    "    plt.xlabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed132419",
   "metadata": {},
   "source": [
    "#### Visualizing with histograms\n",
    "\n",
    "Here's a comparison of the distribution of *et* in oracular speech versus in the \"taunt\" and \"challenge\" categories combined.\n",
    "\n",
    "The distributions overlap, but the taunt/challenge group tends to use *et* more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15944d33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat = 'et'\n",
    "label = 'oracular speech'\n",
    "comp_mask = (x_log.tags=='tau')|(x_log.tags=='cha')\n",
    "comp_label = 'taunt, challenge'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(x_log[x_log.tags=='ora'][feat], bins=15, alpha=0.5, label=label)\n",
    "ax.hist(x_log[comp_mask][feat], bins=15, alpha=0.5, label=comp_label)\n",
    "ax.legend()\n",
    "ax.set_title(feat)\n",
    "ax.set_xlabel('log term frequency')\n",
    "ax.set_ylabel('speeches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490cb57e",
   "metadata": {},
   "source": [
    "**Visualizing as a 2d feature-space**\n",
    "\n",
    "We can also plot each speech as a point in a cartesian space defined by two features. For example, here we look at the log frequencies of 'et' and 'hic'.\n",
    "\n",
    "Now we're starting to see some separation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f26e59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_x = 'et'\n",
    "feat_y = 'qui'\n",
    "\n",
    "comp_label = 'taunt, challenge'\n",
    "comp_mask = x_log.tags.isin(['cha', 'tau'])\n",
    "\n",
    "targ_label = 'oracular speech'\n",
    "targ_mask = x_log.tags == 'ora'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_log[targ_mask][feat_x], x_log[targ_mask][feat_y], marker='o', linestyle='', label=targ_label)\n",
    "ax.plot(x_log[comp_mask][feat_x], x_log[comp_mask][feat_y], marker='o', linestyle='', label=comp_label)\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee51b4d",
   "metadata": {},
   "source": [
    "## What makes oracular speech different?\n",
    "\n",
    "Let's choose a new features set -- this time we'll use all the words that occur in oracular speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b252f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ora_lems = all_tokens.explode('tags').groupby('tags').get_group('ora').lem.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b436e",
   "metadata": {},
   "source": [
    "Now we redo the feature vectors using these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644f3e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# subset the complete vector space\n",
    "x = all_vec.loc[:, ora_lems]\n",
    "\n",
    "# add tags and explode\n",
    "x['tags'] = all_labels.tags\n",
    "x = x.explode('tags')\n",
    "\n",
    "# take the log of all frequencies\n",
    "x_log = x.copy()\n",
    "for col in x_log.columns:\n",
    "    if col != 'tags':\n",
    "        x_log[col] = x[col].apply(np.log).values\n",
    "x_log[x==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c35348",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat = 'fero'\n",
    "label = 'oracular speech'\n",
    "targ_mask = x_log.tags=='ora'\n",
    "comp_mask = x_log.tags=='del'\n",
    "comp_label = 'deliberation'\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(x_log[targ_mask.values][feat], bins=15, alpha=0.5, label=label)\n",
    "ax.hist(x_log[comp_mask.values][feat], bins=15, alpha=0.5, label=comp_label)\n",
    "ax.legend()\n",
    "ax.set_title(feat)\n",
    "ax.set_xlabel('log term frequency')\n",
    "ax.set_ylabel('speeches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b7dab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_samples = 10\n",
    "a = 0.05\n",
    "\n",
    "sig_feats={}\n",
    "pbar = NotebookPBar(max=len(ora_lems))\n",
    "\n",
    "for feat in ora_lems:\n",
    "    pbar.update()\n",
    "    groups = []\n",
    "    labels = []\n",
    "\n",
    "    for name, df in x_log.groupby('tags'):\n",
    "        vals = df[feat]\n",
    "        vals = vals[vals.notna()]\n",
    "        if len(vals) > min_samples:\n",
    "            groups.append(vals)\n",
    "            labels.append(name)\n",
    "    \n",
    "    if len(groups) < 2:\n",
    "        continue\n",
    "        \n",
    "    # perform omnibus anova first\n",
    "    stat, pval = f_oneway(*groups)\n",
    "    if pval > a:\n",
    "        continue\n",
    "    \n",
    "    # perform pairwise tests\n",
    "    tukey = tukey_hsd(*groups)\n",
    "    for i in range(len(groups)):\n",
    "        for j in range(i):\n",
    "            if tukey.pvalue[i,j] < a:\n",
    "                key = tuple(sorted([labels[i], labels[j]]))\n",
    "                sig_feats.setdefault(key, [])\n",
    "                sig_feats[key].append((feat, round(tukey.pvalue[i,j], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc6eae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(pd.DataFrame(dict(\n",
    "        tag1 = tag1,\n",
    "        tag2 = tag2,\n",
    "        features = [feat for feat, pval in sig_feats[(tag1, tag2)]],\n",
    "    ) for tag1, tag2 in sig_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e3dde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sig_feats[('del', 'exh')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945d442",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745b2c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "foo = all_tokens.explode('tags')\n",
    "foo = foo.loc[foo.pos != 'PUNCT']\n",
    "tag_norm = pd.crosstab(foo.tags, foo.lem, normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136779b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_x = 'nefas'\n",
    "feat_y = 'deus'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tag_norm[feat_x], tag_norm[feat_y], marker='o', linestyle='', ms=2)\n",
    "for x, y, s in zip(tag_norm[feat_x], tag_norm[feat_y], tag_norm.index):\n",
    "    ax.text(x,y,s)\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91a398b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1df5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "foo = pd.concat([all_vec, all_labels.tags], axis=1).explode('tags')\n",
    "labels = foo.tags\n",
    "foo = foo.drop('tags', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c795a25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keyness, _ = chi2(foo, labels=='trag')\n",
    "keyness = pd.Series(keyness, index=foo.columns).sort_values(ascending=False)\n",
    "keyness[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750993b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keyness, _ = chi2(tag_norm, tag_norm.index=='')\n",
    "keyness = pd.Series(keyness, index=tag_norm.columns).sort_values(ascending=False)\n",
    "keyness[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae0a00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "foo = tag_norm.loc[tag_norm.index != 'inv']\n",
    "pca_model = PCA(n_components=3)\n",
    "tag_pca = pca_model.fit_transform(foo)\n",
    "tag_pca = pd.DataFrame(\n",
    "    data=tag_pca,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=tag_norm.index[tag_norm.index!='inv'])\n",
    "tag_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635168d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_x = 'PC1'\n",
    "feat_y = 'PC2'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tag_pca[feat_x], tag_pca[feat_y], marker='o', linestyle='', ms=2)\n",
    "for x, y, s in zip(tag_pca[feat_x], tag_pca[feat_y], tag_pca.index):\n",
    "    ax.text(x,y,s)\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361974e4",
   "metadata": {},
   "source": [
    "### lemma-based PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143eba26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=3)\n",
    "lem_pca = pca_model.fit_transform(x)\n",
    "lem_pca = pd.DataFrame(\n",
    "    data=lem_pca,\n",
    "    columns=['PC1', 'PC2', 'PC3'])\n",
    "lem_pca['tags'] = [getTags(s) for s in flav_speeches]\n",
    "lem_pca = lem_pca.explode('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c195e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_x = 'PC1'\n",
    "feat_y = 'PC3'\n",
    "\n",
    "names = ['ora', 'del', 'res']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for name in names:\n",
    "    selecter = lem_pca.tags==name\n",
    "    ax.plot(lem_pca[selecter][feat_x], lem_pca[selecter][feat_y], marker='o', linestyle='', ms=4, label=tagtype[name][:18])\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "#ax.set_xlim((-0.05, 0.05))\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14a8a7",
   "metadata": {},
   "source": [
    "### Same thing but with Seneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5f1fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tagtype['sen'] = 'Seneca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9df2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sen_rows = pd.DataFrame(buildFeatures(s, target_lems) for s in sen_speeches)\n",
    "sen_rows['tags'] = 'sen'\n",
    "flav_rows = x.copy()\n",
    "flav_rows.index = [s.id for s in flav_speeches]\n",
    "flav_rows['tags'] = [getTags(s) for s in flav_speeches]\n",
    "all_rows = pd.concat([sen_rows, flav_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bceab5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64409bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=3)\n",
    "lem_pca = pca_model.fit_transform(all_rows.drop('tags', axis=1))\n",
    "lem_pca = pd.DataFrame(\n",
    "    data=lem_pca,\n",
    "    columns=['PC1', 'PC2', 'PC3'])\n",
    "lem_pca.index = all_rows.index\n",
    "lem_pca['tags'] = all_rows['tags']\n",
    "lem_pca = lem_pca.explode('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48f8f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lem_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33677145",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feat_x = 'PC1'\n",
    "feat_y = 'PC2'\n",
    "\n",
    "names = ['ora', 'nar', 'sen']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for name in names:\n",
    "    selecter = lem_pca.tags==name\n",
    "    ax.plot(lem_pca[selecter][feat_x], lem_pca[selecter][feat_y], marker='o', linestyle='', ms=4, label=tagtype[name][:18])\n",
    "ax.set_xlabel(feat_x)\n",
    "ax.set_ylabel(feat_y)\n",
    "#ax.set_xlim((-0.05, 0.05))\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d3865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lem_pca[lem_pca.tags=='del'].PC3.hist(alpha=0.5)\n",
    "lem_pca[lem_pca.tags=='sen'].PC3.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af207e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d74a86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
