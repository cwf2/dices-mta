{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08220ede",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59682c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# DICES packages\n",
    "from dicesapi import DicesAPI, SpeechGroup\n",
    "from dicesapi.text import CtsAPI, spacy_load\n",
    "import dicesapi.text\n",
    "\n",
    "# for working with local CTS repositories\n",
    "from MyCapytain.resolvers.cts.local import CtsCapitainsLocalResolver\n",
    "from MyCapytain.resources.prototypes.metadata import UnknownCollection\n",
    "\n",
    "# for analysis\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292755fc-4f63-4535-be41-172f4c0a59de",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352ad13-6493-4d4f-8381-e5ba264e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlightKeywordsFullCltk(speech, token=None, lemma=None):\n",
    "    '''Generate HTML results highlighting words matching token/lemma'''\n",
    "\n",
    "    tok_strings = []\n",
    "\n",
    "    for tok in speech.passage.cltk_doc:\n",
    "        flag = True\n",
    "        if (token is not None) and (tok.string != token):\n",
    "            flag = False\n",
    "        if (lemma is not None) and (tok.lemma != lemma):\n",
    "            flag = False\n",
    "        if flag:\n",
    "            tok_string = f'<span style=\"color:red;font-weight:bold\">{tok.string}</span>'\n",
    "        else:\n",
    "            tok_string = tok.string\n",
    "        tok_strings.append(tok_string)\n",
    "\n",
    "    return ' '.join(tok_strings)\n",
    "\n",
    "def highlightKeywordsCltk(speech, token=None, lemma=None):\n",
    "    '''Generate HTML results highlighting words matching token/lemma'''\n",
    "\n",
    "    if speech.passage.line_array is None:\n",
    "        print(f'no line array: {s}')\n",
    "        return highlightKeywordsFullCltk(speech, token, lemma)\n",
    "\n",
    "    hl_by_line = {}\n",
    "    \n",
    "    for tok in speech.passage.cltk_doc:\n",
    "        flag = True\n",
    "        if (token is not None) and (tok.string != token):\n",
    "            flag = False\n",
    "        if (lemma is not None) and (tok.lemma != lemma):\n",
    "            flag = False\n",
    "        if flag:\n",
    "            try:\n",
    "                l_idx = speech.passage.getLineIndex(tok)\n",
    "                assert l_idx is not None\n",
    "            except:\n",
    "                print(f'cannot get line index: {s}')\n",
    "                return highlightKeywordsFullCltk(speech, token, lemma)\n",
    "            hl_by_line[l_idx] = hl_by_line.get(l_idx, []) + [tok]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for l_idx in hl_by_line:\n",
    "        l_loc = s.passage.line_array[l_idx]['n']\n",
    "        l_string = s.passage.line_array[l_idx]['text']\n",
    "        try:\n",
    "            for tok in reversed(hl_by_line[l_idx]):\n",
    "                l_pos = s.passage.getLinePos(tok)\n",
    "                head = l_string[:l_pos]\n",
    "                tail = l_string[l_pos+len(tok.string):]\n",
    "                tok_string = f'<span style=\"color:red;font-weight:bold\">{tok.string}</span>'\n",
    "                l_string = head + tok_string + tail\n",
    "        except:\n",
    "            print(f'highlighting failed: {s}')\n",
    "            return highlightKeywordsFullCltk(speech, token, lemma)\n",
    "\n",
    "        rows.append(f'<tr><td>{l_loc}</td><td>{l_string}</td></tr>')\n",
    "\n",
    "    html = '<table>' + ''.join(rows) + '</table>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e40cba",
   "metadata": {},
   "source": [
    "### Set up local text repositories\n",
    "\n",
    "Here we clone Christopher's fork of the Perseus Greek and Latin texts, so that we can use a local CTS resolver instead of querying the Perseus server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_names = ['canonical-greekLit', 'canonical-latinLit']\n",
    "\n",
    "print('Checking for local text repositories...')\n",
    "\n",
    "for repo in repo_names:\n",
    "    local_dir = os.path.join('data', repo)\n",
    "    remote_url = f'https://github.com/cwf2/{repo}.git'\n",
    "\n",
    "    if os.path.exists(local_dir):\n",
    "        print(f' - {local_dir} exists!')\n",
    "    else:\n",
    "        print(f' - retrieving {remote_url}')\n",
    "        git.Repo.clone_from(remote_url, local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfae50d",
   "metadata": {},
   "source": [
    "### Connection to DICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DicesAPI(\n",
    "    dices_api = 'https://fierce-ravine-99183-425639eee484.herokuapp.com/api/',\n",
    "    logdetail = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d938",
   "metadata": {},
   "source": [
    "### Set up local CTS connection\n",
    "\n",
    "This is the CTS API, allowing us to retrieve texts by URN. In this example, we not only instantiate a default CTS API, but we also create a local resolver that can serve texts from the local repositories we downloaded in the first cell.\n",
    "\n",
    "We have to do a little surgery to overwrite the default CTS API object's resolver with the local one.\n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"margin:1em 2em\">\n",
    "    <p><strong>Note:</strong> The resolver will generate a lot of errors; these can be ignored unless they pertain to a text you want to retrieve.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to local repos\n",
    "repo_paths = [os.path.join('data', repo) for repo in repo_names]\n",
    "\n",
    "# create a local resolver\n",
    "local_resolver = CtsCapitainsLocalResolver(repo_paths)\n",
    "\n",
    "# initialize the CTS API\n",
    "cts = CtsAPI(dices_api = api)\n",
    "\n",
    "# overwrite the default resolver\n",
    "cts._resolvers = {None: local_resolver}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19935066",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### Download the entire DICES dataset\n",
    "\n",
    "We'll start by downloading records for all the speeches in DICES. Then we can select the mother speeches locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches = api.getSpeeches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7272b",
   "metadata": {},
   "source": [
    "#### ⚠️ Workaround for certain Perseus texts\n",
    "\n",
    "These texts have an extra hierarchical level inserted into their loci on Perseus' CTS server. This is a temporary workaround to convert our loci to a form that the server understands.\n",
    "\n",
    "Because `all_speeches` and `mother_speeches` just contain pointers to the same object pool, we can do this modification once on `all_speeches` and the mother speeches will also be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_book_line = [\n",
    "    'De Raptu Proserpinae',\n",
    "    'In Rufinum',\n",
    "]\n",
    "adj_line = [\n",
    "    'Panegyricus de consulatu Manlii Theodori',\n",
    "    'Panegyricus de Tertio Consulatu Honorii Augusti',\n",
    "    'Panegyricus de Sexto Consulatu Honorii Augusti',\n",
    "    'Epithalamium de Nuptiis Honorii Augusti',\n",
    "    'De Bello Gothico',\n",
    "    'Psychomachia',    \n",
    "]\n",
    "\n",
    "for s in all_speeches:\n",
    "    if s.work.title in adj_book_line:\n",
    "        m = re.fullmatch(r'(\\d+)\\.(\\d+)', s.l_fi)\n",
    "        if m:\n",
    "            s.l_fi = f'{m.group(1)}.1.{m.group(2)}'\n",
    "\n",
    "        m = re.fullmatch(r'(\\d+)\\.(\\d+)', s.l_la)\n",
    "        if m:\n",
    "            s.l_la = f'{m.group(1)}.1.{m.group(2)}'\n",
    "\n",
    "    elif s.work.title in adj_line:\n",
    "        m = re.fullmatch(r'(\\d+)', s.l_fi)\n",
    "        if m:\n",
    "            s.l_fi = '1.' + m.group(1)\n",
    "\n",
    "        m = re.fullmatch(r'(\\d+)', s.l_la)\n",
    "        if m:\n",
    "            s.l_la = '1.' + m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2bc95-c626-4455-8e53-d97ac25bcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust loci for perseus editions\n",
    "\n",
    "errata_file = os.path.join('data', 'changed_loci.txt')\n",
    "errata = pd.read_csv(errata_file, sep='\\t', dtype=str)\n",
    "errata = dict([\n",
    "    (f'{row.author} {row.work} {row.l_fi_old}-{row.l_la_old}', (row.l_fi_new, row.l_la_new))\n",
    "    for row in errata.itertuples()])\n",
    "\n",
    "for s in all_speeches:\n",
    "    key = f'{s.author.name} {s.work.title} {s.l_range}'\n",
    "    if key in errata:\n",
    "        print(f'Corrected {s}', end=' ')\n",
    "        s.l_fi, s.l_la = errata[key]\n",
    "        print(f'to {s}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac54dd8",
   "metadata": {},
   "source": [
    "## Get the text\n",
    "\n",
    "Because we're retrieving the texts from a local repository I've turned off caching to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for i, s in enumerate(all_speeches):\n",
    "    if (i % 200 == 0) or (i == len(all_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(all_speeches))} % complete', end='')\n",
    "    if not hasattr(s, 'passage') or s.passage is None:\n",
    "        try:\n",
    "            s.passage = cts.getPassage(s, cache=False)\n",
    "        except:\n",
    "            s.passage = None\n",
    "    if s.passage is None:\n",
    "        failed.append(s)\n",
    "\n",
    "print()\n",
    "print (f'{len(failed)} failed:')\n",
    "for s in failed:\n",
    "    print(f'\\t{s.author.name} {s.work.title} {s.l_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad415e9e",
   "metadata": {},
   "source": [
    "### Add supplementary text for speeches not in Perseus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f40fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'supp_mother_speeches.txt')\n",
    "\n",
    "with open(path) as f:\n",
    "    supplement = json.load(f)\n",
    "\n",
    "for rec in supplement:\n",
    "    for s in all_speeches:\n",
    "        if s.id == rec['id']:\n",
    "            print(s)\n",
    "            s.passage = dicesapi.text.Passage()\n",
    "            s.passage.line_array = rec['line_array']\n",
    "            s.passage._line_index = []\n",
    "            cumsum = 0\n",
    "            for i in range(len(s.passage.line_array)):\n",
    "                s.passage._line_index.append(cumsum)\n",
    "                cumsum += len(s.passage.line_array[i]['text']) + 1\n",
    "            s.passage.text = ' '.join([l['text'] for l in s.passage.line_array])\n",
    "            s.passage.speech = s\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2318e",
   "metadata": {},
   "source": [
    "### Remove speeches with no text available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_speeches = all_speeches.advancedFilter(lambda s: s.passage is not None).sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e5a54",
   "metadata": {},
   "source": [
    "### Read the list of mother-child pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "mothers_file = os.path.join('data', 'mother-child.csv')\n",
    "mothers = pd.read_csv(mothers_file, sep='\\t')\n",
    "display(mothers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922e37e",
   "metadata": {},
   "source": [
    "### Identify mother speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af85a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motherValidation(speech):\n",
    "    '''check whether any speaker-addressee combo is in the mother-child list'''\n",
    "    valid_keys = list(mothers.spkr + ':' + mothers.addr)\n",
    "\n",
    "    for spkr in speech.spkr:\n",
    "        for addr in speech.addr:\n",
    "            key = f'{spkr.name}:{addr.name}'\n",
    "            if key in valid_keys:\n",
    "                return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_speeches = all_speeches.advancedFilter(motherValidation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc9184",
   "metadata": {},
   "source": [
    "### Class as mother/non-mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in all_speeches:\n",
    "    if s in mother_speeches:\n",
    "        s.is_mother = True\n",
    "    else:\n",
    "        s.is_mother = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_mother = len([s for s in test_speeches if s.is_mother])\n",
    "kept_non_mother = len([s for s in test_speeches if not s.is_mother])\n",
    "\n",
    "\n",
    "print(f'{kept_mother}/{len(mother_speeches)} mother speeches selected')\n",
    "print(f'{kept_non_mother}/{len(all_speeches) - len(mother_speeches)} non-mother speeches selected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35fb75",
   "metadata": {},
   "source": [
    "# Run NLP\n",
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spacy models\n",
    "spacy_load(\n",
    "    latin_model = 'la_core_web_lg',\n",
    "    greek_model = 'grc_odycy_joint_trf',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for i, s in enumerate(test_speeches):\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "    if s.passage.spacy_doc is None:\n",
    "        s.passage.runSpacyPipeline()\n",
    "    if s.passage.spacy_doc is None:\n",
    "        failed.append(s)\n",
    "\n",
    "if len(failed) > 0:\n",
    "    print(f'SpaCy failed for {len(failed)} speeches:')\n",
    "    for s in failed:\n",
    "        print(f' - {s.work.urn}\\t{s.work.title}\\t{s.l_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64024ab0",
   "metadata": {},
   "source": [
    "### Generate tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = []\n",
    "\n",
    "# extract features\n",
    "for i, s in enumerate(test_speeches):\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "    for tok in s.passage.spacy_doc:\n",
    "        spacy_tokens.append(dict(\n",
    "            speech_id = s.id,\n",
    "            lang = s.lang,\n",
    "            author = s.author.name,\n",
    "            work = s.work.title,\n",
    "            l_fi = s.l_fi,\n",
    "            l_la = s.l_la,\n",
    "            nlines = len(s.passage.line_array),\n",
    "            spkr = ','.join([inst.name for inst in s.spkr]),\n",
    "            addr = ','.join([inst.name for inst in s.addr]),\n",
    "            part = s.part,\n",
    "            mother = s.is_mother,\n",
    "            line = s.passage.line_array[s.passage.getLineIndex(tok)]['n'],\n",
    "            token = tok.text,\n",
    "            lemma = tok.lemma_,\n",
    "            pos = tok.pos_,\n",
    "            mood = tok.morph.get('Mood'),\n",
    "            tense = tok.morph.get('Tense'),\n",
    "            voice = tok.morph.get('Voice'),\n",
    "            person = tok.morph.get('Person'),\n",
    "            number = tok.morph.get('Number'),\n",
    "            case = tok.morph.get('Case'),\n",
    "            gender = tok.morph.get('Gender'),\n",
    "            verbform = tok.morph.get('VerbForm'),\n",
    "            degree = tok.morph.get('Degree'),\n",
    "            prontype = tok.morph.get('PronType'),\n",
    "        ))\n",
    "\n",
    "# convert to data frame\n",
    "spacy_tokens = pd.DataFrame(spacy_tokens)\n",
    "\n",
    "# simplify list cells\n",
    "cols = ['mood', 'tense', 'voice', 'person', 'number', 'case', 'gender', 'verbform', 'degree', 'prontype']\n",
    "spacy_tokens[cols] = spacy_tokens[cols].map(lambda x: None if len(x) == 0 else ','.join(x))\n",
    "\n",
    "# display\n",
    "display(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e697d-e0a5-487f-91a5-aca10ca71e88",
   "metadata": {},
   "source": [
    "## CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836a8e7-a8d2-4c40-91d0-4e18d70637aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for i, s in enumerate(test_speeches):\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "\n",
    "    if s.passage.cltk_doc is None:\n",
    "        try:\n",
    "            s.passage.runCltkPipeline()\n",
    "        except:\n",
    "            print(s)\n",
    "            print(s.passage.text)\n",
    "    if s.passage.cltk_doc is None:\n",
    "        failed.append(s)\n",
    "\n",
    "if len(failed) > 0:\n",
    "    print(f'CLTK failed for {len(failed)} speeches:')\n",
    "    for s in failed:\n",
    "        print(f' - {s.work.urn}\\t{s.work.title}\\t{s.l_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dced2b-be7b-439e-af44-4fb0443b595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract CLTK features as strings\n",
    "\n",
    "def getCltkFeature(token, feature, default=None):\n",
    "    '''convert token's feature bundle to a dictionary and perform a get'''\n",
    "    d = dict(zip([str(k) for k in token.features.keys()], token.features.values()))\n",
    "    vlist = d.get(feature)\n",
    "\n",
    "    if vlist is None:\n",
    "        return(default)\n",
    "\n",
    "    return [str(v) for v in vlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62beb68d-37eb-4b6b-bd4c-bf3de454ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_tokens = []\n",
    "\n",
    "# extract features\n",
    "for i, s in enumerate(test_speeches):\n",
    "    # progress\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "\n",
    "    # process all tokens in speech\n",
    "    for tok in s.passage.cltk_doc:\n",
    "        cltk_tokens.append(dict(\n",
    "            speech_id = s.id,\n",
    "            lang = s.lang,\n",
    "            author = s.author.name,\n",
    "            work = s.work.title,\n",
    "            l_fi = s.l_fi,\n",
    "            l_la = s.l_la,\n",
    "            nlines = len(s.passage.line_array),\n",
    "            spkr = ','.join([inst.name for inst in s.spkr]),\n",
    "            addr = ','.join([inst.name for inst in s.addr]),\n",
    "            part = s.part,\n",
    "            mother = s.is_mother,\n",
    "            line = s.passage.line_array[s.passage.getLineIndex(tok)]['n'] if s.passage.getLineIndex(tok) is not None else None,\n",
    "            token = tok.string,\n",
    "            lemma = tok.lemma,\n",
    "            pos = tok.upos,\n",
    "            mood = getCltkFeature(tok, 'Mood'),\n",
    "            tense = getCltkFeature(tok, 'Tense'),\n",
    "            voice = getCltkFeature(tok, 'Voice'),\n",
    "            aspect = getCltkFeature(tok, 'Aspct'),\n",
    "            person = getCltkFeature(tok, 'Person'),\n",
    "            number = getCltkFeature(tok, 'Number'),\n",
    "            case = getCltkFeature(tok, 'Case'),\n",
    "            gender = getCltkFeature(tok, 'Gender'),\n",
    "            degree = getCltkFeature(tok, 'Degree'),\n",
    "            verbform = getCltkFeature(tok, 'VerbForm'),    \n",
    "        ))\n",
    "\n",
    "cltk_tokens = pd.DataFrame(cltk_tokens)\n",
    "\n",
    "# simplify list cells\n",
    "cols = ['mood', 'tense', 'voice', 'aspect', 'person', 'number', 'case', 'gender', 'degree', 'verbform']\n",
    "cltk_tokens[cols] = cltk_tokens[cols].map(lambda x: None if x is None else ','.join(x))\n",
    "\n",
    "# display results\n",
    "display(cltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760370da-51c9-4ad5-9fef-20f331cff3ea",
   "metadata": {},
   "source": [
    "## Deduplicate embedded lines\n",
    "\n",
    "Keep only the most embedded instance of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14410cb5-a024-4fea-96fc-64177ce851ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cltk_tokens.loc[(cltk_tokens['work']=='Odyssey') & cltk_tokens['l_fi'].str.startswith('9.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd532c-c503-4458-a364-4e0e4eeccb50",
   "metadata": {},
   "source": [
    "## Load hand-selected feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883db4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = '/Users/chris/Dropbox/Epic Speeches/Listen to mummy/classification.xlsx'\n",
    "lemma_class = pd.read_excel(excel_file, 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_dict = dict()\n",
    "\n",
    "for label in lemma_class.label.unique():\n",
    "    if not pd.isna(label):\n",
    "        lem_dict[label] = lemma_class.loc[lemma_class.label == label, 'lemma'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690687e4-17f1-4d7f-bbec-5058325e1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfdb87-efb8-4357-ac9c-60dbf8e229bb",
   "metadata": {},
   "source": [
    "### Classify lemmata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08007f-8cfd-4c7c-914e-b4b594709c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in ['family', 'pers_poss_s', 'pers_poss_p']:\n",
    "    spacy_tokens[tag] = spacy_tokens['lemma'].isin(lem_dict[tag])\n",
    "    cltk_tokens[tag] = cltk_tokens['lemma'].isin(lem_dict[tag])\n",
    "cltk_tokens['marked_verb'] = cltk_tokens['mood'].isin(['optative', 'subjunctive'])\n",
    "spacy_tokens['marked_verb'] = spacy_tokens['mood'].isin(['Opt', 'Sub'])\n",
    "# cltk_tokens['marked_verb'] = (cltk_tokens['mood'].isin(['optative', 'subjunctive', 'imperative']) | \n",
    "#                               cltk_tokens['tense'] == 'future')\n",
    "# spacy_tokens['marked_verb'] = (spacy_tokens['mood'].isin(['Opt', 'Sub', 'Imp',]) | \n",
    "#                               spacy_tokens['tense'] == 'Fut')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbcc6f-ff54-4d2c-b927-40ab3b481e48",
   "metadata": {},
   "source": [
    "## Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bac91-f62b-44f0-9c74-ca5b5b474ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens.to_csv(os.path.join('data', 'spacy_token_table.csv'), index=False)\n",
    "cltk_tokens.to_csv(os.path.join('data', 'cltk_token_table.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb577b81-4ebd-4a47-808a-24ec80415541",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "### Speech labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fa576",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tokens.groupby('speech_id').agg(\n",
    "    lang = ('lang', 'first'),\n",
    "    author = ('author', 'first'),\n",
    "    work = ('work', 'first'),\n",
    "    l_fi = ('l_fi', 'first'),\n",
    "    l_la = ('l_la', 'first'),    \n",
    "    spkr = ('spkr', 'first'),\n",
    "    addr = ('addr', 'first'),\n",
    "    part = ('part', 'first'),\n",
    "    nlines = ('nlines', 'first'),\n",
    "    mother = ('mother', 'first'),\n",
    "    ntokens = ('token', 'count'),\n",
    ")\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9b077-4b82-419b-85b9-20d0b319a5d1",
   "metadata": {},
   "source": [
    "### Mother/non-mother by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.groupby(['lang', 'mother', 'author']).agg(\n",
    "    speeches = ('l_fi', 'count'),\n",
    "    lines = ('nlines', 'sum'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7038d-feb7-4b01-8853-f968d9f5239c",
   "metadata": {},
   "source": [
    "### By language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fcb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.groupby(['mother', 'lang']).agg(\n",
    "    speeches = ('speech_id', 'nunique'),\n",
    "    tokens = ('token', 'count'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92164229-49b9-45cf-a972-45e9a315ef73",
   "metadata": {},
   "source": [
    "## Distribution of morphological features\n",
    "\n",
    "### Moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b0cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tokens.mood.isin(['Imp', 'Ind', 'Opt', 'Sub'])\n",
    "grouped = tokens[mask].groupby('lang')\n",
    "\n",
    "for name, group in grouped:\n",
    "    df = pd.crosstab(group.mother, group.mood, normalize='index')\n",
    "    display(name, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab872a-945c-4b99-b4f1-72f2c3238136",
   "metadata": {},
   "source": [
    "### Person and number (finite verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90831cf4-3aff-48df-877e-664fe5dc87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tokens.mood.isin(['Imp', 'Ind', 'Opt', 'Sub'])\n",
    "grouped = tokens[mask].groupby('lang')\n",
    "\n",
    "for name, group in grouped:\n",
    "    df = pd.crosstab(group.mother, group.person + group.number, normalize='index')\n",
    "    display(name, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a7838",
   "metadata": {},
   "source": [
    "### Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in tokens.groupby('lang'):\n",
    "    df = pd.crosstab(group.mother, group.prontype, normalize='index')\n",
    "    display(name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768d9ca-ee45-42dd-a660-f7e22bd8b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokens.loc[tokens.prontype=='Prs']\n",
    "for name, group in x.groupby(x.mother):\n",
    "    print(name)\n",
    "    df = pd.crosstab(group.number, group.person)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_dict['giving']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a25739",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['family'] = tokens['lemma'].isin(lem_dict['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e672f8f-255d-4680-84ba-9b9b5eaed862",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokens.lang=='greek'\n",
    "pd.crosstab(tokens.loc[x]['mother'], tokens.loc[x]['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4ee18-e10c-4357-a7f0-ecc9b8fe51af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokens.lang=='greek'\n",
    "pd.crosstab(tokens.loc[x]['mother'], tokens.loc[x]['family'], normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7021826",
   "metadata": {},
   "source": [
    "## Full feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table = (\n",
    "    cltk_token_table.pivot_table(\n",
    "        index = 'speech_id',\n",
    "        columns = 'theme',\n",
    "        values = 'token',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    )\n",
    "    .drop('', axis=1)\n",
    ".join(\n",
    "    cltk_token_table.pivot_table(\n",
    "        index = 'speech_id',\n",
    "        columns = 'mood',\n",
    "        values = 'token',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    )\n",
    "    .assign(\n",
    "        subopt = lambda x: x['subjunctive'] + x['optative'])\n",
    "    \n",
    "    .drop(['', 'subjunctive', 'optative', 'gerund', 'gerundive'], axis=1)\n",
    ")\n",
    ".join(\n",
    "    cltk_token_table.pivot_table(\n",
    "        index = 'speech_id',\n",
    "        columns = 'pers',\n",
    "        values = 'token',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    )\n",
    "    .drop(['', 'first', 'plural', 'second', 'singular'], axis=1)\n",
    ")\n",
    ".join(\n",
    "    cltk_token_table.pivot_table(\n",
    "        index = 'speech_id',\n",
    "        columns = 'pron',\n",
    "        values = 'token',\n",
    "        aggfunc = 'count',\n",
    "        fill_value = 0,\n",
    "    ).loc[:,['interrogative', 'personal', 'reciprocal', 'relative']]\n",
    ")\n",
    ".join( \n",
    "    cltk_token_table.groupby('speech_id')['poss']\n",
    "    .value_counts()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    "    .rename(columns={'pos':'possessive'})['possessive']\n",
    ")\n",
    ".div(labels.ntokens, axis=0)\n",
    ")\n",
    "\n",
    "feature_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c70bca",
   "metadata": {},
   "source": [
    "### Log frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = feature_table.apply(np.log)\n",
    "x[feature_table==0] = np.nan\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67270884",
   "metadata": {},
   "source": [
    "### Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=3)\n",
    "\n",
    "pca = pd.DataFrame(\n",
    "    index = feature_table.index,\n",
    "    data = pca_model.fit_transform(feature_table), \n",
    "    columns=['PC1', 'PC2', 'PC3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3797584",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e16bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'family'\n",
    "label = 'mother'\n",
    "\n",
    "groups = x.groupby(labels[label])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(\n",
    "    [group[feat].dropna() for name, group in groups])\n",
    "ax.set_xticks([j + 1 for j in range(len(groups))],\n",
    "    labels = [name for name, group in groups])\n",
    "ax.set_xlabel(label)\n",
    "ax.set_ylabel('log frequency')\n",
    "ax.set_title(f'{feat} vocabulary')\n",
    "plt.savefig(f'{feat}_box.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42904c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'family'\n",
    "label = 'mother'\n",
    "\n",
    "groups = x.groupby(labels[label])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.violinplot(\n",
    "    [group[feat].dropna() for name, group in groups], showmeans=True)\n",
    "ax.set_xticks([j + 1 for j in range(len(groups))],\n",
    "    labels = [name for name, group in groups])\n",
    "ax.set_xlabel(label)\n",
    "ax.set_ylabel('log frequency')\n",
    "ax.set_title(f'{feat} vocabulary')\n",
    "plt.savefig(f'{feat}_box.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat = 'family'\n",
    "y_feat = 'giving'\n",
    "label = 'mother'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label_value in labels[label].unique():\n",
    "    mask = labels[label]==label_value\n",
    "    ax.loglog(feature_table.loc[mask, x_feat], feature_table.loc[mask, y_feat], marker='o', linestyle='', label=label_value)\n",
    "ax.set_xlabel(x_feat)\n",
    "ax.set_ylabel(y_feat)\n",
    "ax.legend(title=label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat = 'personal'\n",
    "y_feat = 'possessive'\n",
    "label = 'lang'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label_value in labels[label].unique():\n",
    "    mask = labels[label]==label_value\n",
    "    ax.loglog(feature_table.loc[mask, x_feat], feature_table.loc[mask, y_feat], marker='o', linestyle='', label=label_value)\n",
    "ax.set_xlabel(x_feat)\n",
    "ax.set_ylabel(y_feat)\n",
    "ax.legend(title=label)\n",
    "ax.set_title('pronouns in mother-child speeches')\n",
    "plt.savefig('pron.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat = 'PC1'\n",
    "y_feat = 'PC2'\n",
    "label = 'lang'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label_value in labels[label].unique():\n",
    "    mask = labels[label]==label_value\n",
    "    ax.plot(pca.loc[mask, x_feat], pca.loc[mask, y_feat], marker='o', linestyle='', label=label_value)\n",
    "ax.set_xlabel(x_feat)\n",
    "ax.set_ylabel(y_feat)\n",
    "ax.legend(title=label)\n",
    "ax.set_title(f'Principal Components from {len(feature_table.columns)} parameters')\n",
    "plt.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78287bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat = 'PC1'\n",
    "y_feat = 'PC2'\n",
    "label = 'auth'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label_value in ['Homer', 'Apollonius', 'Virgil', 'Nonnus']:\n",
    "    mask = (labels[label]==label_value) & labels.mother\n",
    "    ax.plot(pca.loc[mask, x_feat], pca.loc[mask, y_feat], marker='o', ls='', label=label_value)\n",
    "ax.set_xlabel(x_feat)\n",
    "ax.set_ylabel(y_feat)\n",
    "ax.legend(title=label)\n",
    "ax.set_title(f'Principal Components from {len(feature_table.columns)} parameters')\n",
    "plt.savefig('foo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd79052",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef2152-98ac-4dd0-97c8-c5b544ece65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intj_list = cltk_tokens.loc[cltk_tokens['pos']=='INTJ']['lemma'].unique()\n",
    "cltk_tokens.loc[cltk_tokens['lemma'].isin(intj_list)].groupby('token').agg(\n",
    "    lemma = ('lemma', set),\n",
    "    count = ('l_fi', 'count'),\n",
    ").sort_values('count', ascending=False).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb51585-f320-47ba-bffd-31c6b8b6d8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82caab1-c862-4ea5-a485-aeb6e089fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_list = cltk_tokens.loc[(cltk_tokens['token']=='ἦ') & (cltk_tokens['lemma']=='ἤ')]['speech_id'].unique()\n",
    "html = ''\n",
    "\n",
    "for s in test_speeches.filterIDs(speech_list):\n",
    "    tok_strings = []\n",
    "    for tok in s.passage.cltk_doc:\n",
    "        if (tok.string == 'ἦ') & (tok.lemma == 'ἤ'):\n",
    "            tok_string = f'<span style=\"color:red\">{tok.string}</span>'\n",
    "        else:\n",
    "            tok_string = tok.string\n",
    "        tok_strings.append(tok_string)\n",
    "\n",
    "    html += '<div>'\n",
    "    html += f'<h3>{s.author.name} {s.work.title} {s.l_range}: {s.getSpkrString()} to {s.getAddrString()}</h3>'\n",
    "    html += highlightKeywordsCltk(s, token='ἦ', lemma='ἤ')\n",
    "    html += '</div>\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad5220-04be-4041-bc10-cd29bfad435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = [s.passage.cltk_doc[17],s.passage.cltk_doc[14]]\n",
    "l_idx = s.passage.getLineIndex(toks[0])\n",
    "line_string = s.passage.line_array[l_idx]['text']\n",
    "for tok in toks:\n",
    "    l_pos = s.passage.getLinePos(tok)\n",
    "    line_string = line_string[:l_pos] + '[' + tok.string + ']' + line_string[l_pos+len(tok.string):]\n",
    "print(line_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6ec27-d17a-492e-ab6b-524bd64b5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76117d-f3ca-463e-982a-82e28809a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_lem = spacy_tokens['lemma'].isin(lem_dict['family'])\n",
    "is_tok = spacy_tokens['token'].isin(lem_dict['family'])\n",
    "lem_or_tok = is_lem | is_tok\n",
    "spacy_fam = spacy_tokens.loc[lem_or_tok].groupby(['lemma', 'token']).agg(\n",
    "    count = ('token', 'count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43be2dc-5c21-485c-a6d6-03ef9ec62f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_lem = cltk_tokens['lemma'].isin(lem_dict['family'])\n",
    "is_tok = cltk_tokens['token'].isin(lem_dict['family'])\n",
    "lem_or_tok = is_lem | is_tok\n",
    "cltk_fam = cltk_tokens.loc[lem_or_tok].groupby(['lemma', 'token']).agg(\n",
    "    count = ('token', 'count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16aeac8-2e6a-4cd8-b628-329fdf386956",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spacy_fam.join(cltk_fam, how='outer', lsuffix='_spacy', rsuffix='_cltk')\n",
    "         .fillna(0)\n",
    "         .astype(int)\n",
    ").to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe5cf0-4324-40a8-9550-0b81a291831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_tokens.loc[cltk_tokens['token']=='τοκῆος'].groupby('lemma').agg(count=('token','count')).sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc14175-4466-4498-87be-b034fa5a5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens.loc[spacy_tokens['token']=='γενέτη']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b0e18-a641-47e8-9730-4dfa5d8c803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cltk_tokens.mother, cltk_tokens['mood']=='imperative', normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b12e29-bf0c-4f94-9edc-5ea36c3cd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(spacy_tokens.mother, spacy_tokens['mood']=='Imp', normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc97c2-653d-4ed9-a9b6-8bf3036bedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cltk_tokens.groupby('speech_id').agg(\n",
    "    lang = ('lang', 'first'),\n",
    "    author = ('author', 'first'),\n",
    "    work = ('work', 'first'),\n",
    "    l_fi = ('l_fi', 'first'),\n",
    "    l_la = ('l_la', 'first'),\n",
    "    spkr = ('spkr', 'first'),\n",
    "    addr = ('addr', 'first'),    \n",
    "    mother = ('mother', 'first'),\n",
    "    family = ('family', 'sum'),\n",
    "    pers_poss_s = ('pers_poss_s', 'sum'),\n",
    "    marked_verb = ('marked_verb', 'sum'),\n",
    "    tokens = ('token', 'count'),\n",
    ")\n",
    "x['family'] = x['family'].div(x['tokens'])\n",
    "x['pers_poss_s'] = x['pers_poss_s'].div(x['tokens'])\n",
    "x['marked_verb'] = x['marked_verb'].div(x['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04d4e3-d9d5-4a5b-bcb2-5d7d336d4601",
   "metadata": {},
   "source": [
    "x[['mother', 'family', 'pers_poss_s', 'marked_verb']].plot.box('mother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af2e50-d9bd-43a8-a4e9-e5ce7ad92bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_tokens.marked_verb.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2edc4-77ee-41cf-a4b6-49f5c2555321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang, group in cltk_tokens.groupby('speech_id').agg(\n",
    "    lang = ('lang', 'first'),\n",
    "    mother = ('mother', 'first'),\n",
    "    family = ('family', 'sum'),\n",
    "    pers_poss_s = ('pers_poss_s', 'sum'),\n",
    "    marked_verb = ('marked_verb', 'sum'),\n",
    "    tokens = ('token', 'count'),\n",
    ").groupby('lang'):\n",
    "    xs = group['family'].div(group['tokens'])\n",
    "    ys = (group['marked_verb'] + group['pers_poss_s']).div(group['tokens'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xs[~x['mother']], ys[~x['mother']], \n",
    "        marker='o', ls='', label='non-mother')\n",
    "    ax.plot(xs[x['mother']], ys[x['mother']], \n",
    "        marker='o', ls='', label='mother')\n",
    "    ax.loglog()\n",
    "    ax.set_title(lang)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10235a-6149-4206-93b6-d7875a59d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c0cb2-79a9-4063-b7cd-26d1d55b945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(x['mother'], x['family'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2dcca-51a2-4865-b58b-585a98501034",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[x['mother'] & (x['family'] + x['pers_poss_s'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b51c0a-77ac-41c3-937a-abfe2c58fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = x.loc[x['mother'] & (x['family'] + x['pers_poss_s'] == 0)].index.values\n",
    "cltk_tokens.loc[cltk_tokens['speech_id'].isin(zeros)].groupby('speech_id').agg(\n",
    "    author = ('author', 'first'),\n",
    "    work = ('work', 'first'),\n",
    "    l_fi = ('l_fi', 'first'),\n",
    "    l_la = ('l_la', 'first'),\n",
    "    spkr = ('spkr', 'first'),\n",
    "    addr = ('addr', 'first'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee036f-aa8d-4de7-9e4f-7d555575c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens['tense'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e84160-0796-4b9a-8a3c-e0139b51e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_tokens['tense'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e3a04-b0a0-4d90-aef0-4b6243a286c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cltk_tokens['marked_verb'] = cltk_tokens['mood'].isin(['subjunctive', 'optative', 'imperative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0264a-0056-47ca-809b-db5b32dfeeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[x['spkr']=='Maria'].sort_values('family', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa966a67-ff0a-4dba-b5c0-1326acb85998",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[x['mother']].sort_values('family', ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07951a37-725f-417d-b81e-7952d229baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens.loc[spacy_tokens['mother']].groupby('speech_id').agg(\n",
    "    lang = ('lang', 'first'),\n",
    "    author = ('author', 'first'),\n",
    "    work = ('work', 'first'),\n",
    "    l_fi = ('l_fi', 'first'),\n",
    "    l_la = ('l_la', 'first'),\n",
    "    spkr = ('spkr', 'first'),\n",
    "    addr = ('addr', 'first'),    \n",
    "    mother = ('mother', 'first'),\n",
    "    family = ('family', 'sum'),\n",
    "    pers_poss_s = ('pers_poss_s', 'sum'),\n",
    "    marked_verb = ('marked_verb', 'sum'),\n",
    "    tokens = ('token', 'count'),\n",
    "    lines = ('line', 'nunique'),\n",
    ").sort_values('lines', ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247e211-e86d-462c-b950-8c34a974b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens.loc[spacy_tokens['mother']]['speech_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33960c-1ebf-4eba-9ef9-e5388700795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['pers_poss', 'family']\n",
    "for speech_id in spacy_tokens.loc[spacy_tokens['mother']]['speech_id'].unique():\n",
    "    x = (spacy_tokens\n",
    "        .loc[(spacy_tokens['speech_id']==speech_id) & (spacy_tokens['pos']!='PUNCT')]\n",
    "        .groupby('line')\n",
    "        .agg(\n",
    "            family = ('family', 'sum'),\n",
    "            pers_poss = ('pers_poss', 'sum'),\n",
    "            marked_verb = ('marked_verb', 'sum'),\n",
    "            tokens = ('token', 'count'),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    labels = (spacy_tokens\n",
    "        .loc[spacy_tokens['speech_id']==speech_id]\n",
    "        .groupby('speech_id')\n",
    "        .agg(\n",
    "            author = ('author', 'first'),\n",
    "            work = ('work', 'first'),\n",
    "            l_fi = ('l_fi', 'first'),\n",
    "            l_la = ('l_la', 'first'),\n",
    "            spkr = ('spkr', 'first'),\n",
    "            addr = ('addr', 'first'),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    title = f\"{labels.iloc[0]['spkr']} to {labels.iloc[0]['addr']}\"\n",
    "    subtitle = f\"{labels.iloc[0]['author']} {labels.iloc[0]['work']} {labels.iloc[0]['l_fi']}-{labels.iloc[0]['l_la']}\"\n",
    "\n",
    "    try:\n",
    "        xs = x.index.values.astype('int')\n",
    "    except:\n",
    "        xs = range(1, len(x.index.values)+1)\n",
    "\n",
    "    if len(xs) < 5:\n",
    "        continue\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    for feat in feats:\n",
    "        ax.bar(xs, x[feat], label=feat)\n",
    "    if len(xs) < 15:\n",
    "        ax.set_xticks(xs)\n",
    "    ax.set_xlabel('line')\n",
    "    ax.set_ylabel('tokens')\n",
    "    fig.suptitle(title)\n",
    "    ax.set_title(subtitle)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig(os.path.join('fig', f'fig_{speech_id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13112b-d43c-4066-9d97-f5640d296b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.iloc[0]['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda3daf-6c57-4870-a0b0-0fc441f3e465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
