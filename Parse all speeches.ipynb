{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08220ede",
   "metadata": {
    "id": "08220ede"
   },
   "source": [
    "# Preliminaries\n",
    "\n",
    "### Install prerequisites\n",
    "This is necessary for Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x3hMdqpQuwo0",
   "metadata": {
    "id": "x3hMdqpQuwo0"
   },
   "outputs": [],
   "source": [
    "# python packages\n",
    "!pip install -q GitPython MyCapytain\n",
    "!pip install -q git+https://github.com/cwf2/dices-client\n",
    "\n",
    "# language models\n",
    "!pip install -q https://huggingface.co/latincy/la_core_web_lg/resolve/1973a08557127e1d306ab70239bfb73f560a8cb4/la_core_web_lg-any-py3-none-any.whl\n",
    "!pip install -q https://huggingface.co/chcaa/grc_odycy_joint_trf/resolve/main/grc_odycy_joint_trf-any-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_3OVA7rupv2",
   "metadata": {
    "id": "h_3OVA7rupv2"
   },
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59682c6",
   "metadata": {
    "editable": true,
    "id": "c59682c6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import git\n",
    "import requests\n",
    "\n",
    "# DICES packages\n",
    "from dicesapi import DicesAPI, SpeechGroup\n",
    "from dicesapi.text import CtsAPI, spacy_load\n",
    "import dicesapi.text\n",
    "\n",
    "# for working with local CTS repositories\n",
    "from MyCapytain.resolvers.cts.local import CtsCapitainsLocalResolver\n",
    "from MyCapytain.resources.prototypes.metadata import UnknownCollection\n",
    "\n",
    "# for analysis\n",
    "import pandas as pd\n",
    "\n",
    "# verbose output\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o-sQyVUsRpxX",
   "metadata": {
    "id": "o-sQyVUsRpxX"
   },
   "source": [
    "### Download data files\n",
    "\n",
    "Also necessary for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LN7w11tVRw-x",
   "metadata": {
    "id": "LN7w11tVRw-x"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "for filename in [\"changed_loci.txt\", \"supp_mother_speeches.txt\"]:\n",
    "    path = os.path.join('data', filename)\n",
    "    if not os.path.exists(path):\n",
    "        res = requests.get(f'https://raw.githubusercontent.com/cwf2/dices-mta/main/data/{filename}')\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e40cba",
   "metadata": {
    "id": "a8e40cba"
   },
   "source": [
    "### Set up local text repositories\n",
    "\n",
    "Here we clone Christopher's fork of the Perseus Greek and Latin texts, so that we can use a local CTS resolver instead of querying the Perseus server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618943b",
   "metadata": {
    "id": "0618943b"
   },
   "outputs": [],
   "source": [
    "repo_names = ['canonical-greekLit', 'canonical-latinLit']\n",
    "\n",
    "print('Checking for local text repositories...')\n",
    "\n",
    "for repo in repo_names:\n",
    "    local_dir = os.path.join('data', repo)\n",
    "    remote_url = f'https://github.com/cwf2/{repo}.git'\n",
    "\n",
    "    if os.path.exists(local_dir):\n",
    "        print(f' - {local_dir} exists!')\n",
    "    else:\n",
    "        print(f' - retrieving {remote_url}')\n",
    "        git.Repo.clone_from(remote_url, local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfae50d",
   "metadata": {
    "id": "2cfae50d"
   },
   "source": [
    "### Connection to DICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9e9f0",
   "metadata": {
    "id": "b9c9e9f0"
   },
   "outputs": [],
   "source": [
    "api = DicesAPI(\n",
    "    logfile = 'dices.log',\n",
    "    logdetail = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d938",
   "metadata": {
    "id": "f529d938"
   },
   "source": [
    "### Set up local CTS connection\n",
    "\n",
    "This is the CTS API, allowing us to retrieve texts by URN. In this example, we not only instantiate a default CTS API, but we also create a local resolver that can serve texts from the local repositories we downloaded in the first cell.\n",
    "\n",
    "We have to do a little surgery to overwrite the default CTS API object's resolver with the local one.\n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"margin:1em 2em\">\n",
    "    <p><strong>Note:</strong> The resolver will generate a lot of errors; these can be ignored unless they pertain to a text you want to retrieve.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27022a",
   "metadata": {
    "id": "ed27022a"
   },
   "outputs": [],
   "source": [
    "# path to local repos\n",
    "repo_paths = [os.path.join('data', repo) for repo in repo_names]\n",
    "\n",
    "# create a local resolver\n",
    "local_resolver = CtsCapitainsLocalResolver(repo_paths, logger=api.log)\n",
    "\n",
    "# initialize the CTS API\n",
    "cts = CtsAPI(dices_api = api)\n",
    "\n",
    "# overwrite the default resolver\n",
    "cts._resolvers = {None: local_resolver}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19935066",
   "metadata": {
    "id": "19935066"
   },
   "source": [
    "# Data\n",
    "\n",
    "### Download the entire DICES dataset\n",
    "\n",
    "We'll start by downloading records for all the speeches in DICES. Then we can select the mother speeches locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f3dc5",
   "metadata": {
    "id": "1b3f3dc5"
   },
   "outputs": [],
   "source": [
    "all_speeches = api.getSpeeches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7272b",
   "metadata": {
    "id": "71a7272b"
   },
   "source": [
    "#### ⚠️ Workaround for certain Perseus texts\n",
    "\n",
    "These texts have an extra hierarchical level inserted into their loci on Perseus' CTS server. This is a temporary workaround to convert our loci to a form that the server understands.\n",
    "\n",
    "Because `all_speeches` and `mother_speeches` just contain pointers to the same object pool, we can do this modification once on `all_speeches` and the mother speeches will also be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823a359",
   "metadata": {
    "id": "2823a359"
   },
   "outputs": [],
   "source": [
    "adj_book_line = [\n",
    "    'De Raptu Proserpinae',\n",
    "    'In Rufinum',\n",
    "]\n",
    "adj_line = [\n",
    "    'Panegyricus de consulatu Manlii Theodori',\n",
    "    'Panegyricus de Tertio Consulatu Honorii Augusti',\n",
    "    'Panegyricus de Sexto Consulatu Honorii Augusti',\n",
    "    'Epithalamium de Nuptiis Honorii Augusti',\n",
    "    'De Bello Gothico',\n",
    "    'Psychomachia',\n",
    "]\n",
    "\n",
    "for s in all_speeches:\n",
    "    if s.work.title in adj_book_line:\n",
    "        m = re.fullmatch(r'(\\d+)\\.(\\d+)', s.l_fi)\n",
    "        if m:\n",
    "            s.l_fi = f'{m.group(1)}.1.{m.group(2)}'\n",
    "\n",
    "        m = re.fullmatch(r'(\\d+)\\.(\\d+)', s.l_la)\n",
    "        if m:\n",
    "            s.l_la = f'{m.group(1)}.1.{m.group(2)}'\n",
    "\n",
    "    elif s.work.title in adj_line:\n",
    "        m = re.fullmatch(r'(\\d+)', s.l_fi)\n",
    "        if m:\n",
    "            s.l_fi = '1.' + m.group(1)\n",
    "\n",
    "        m = re.fullmatch(r'(\\d+)', s.l_la)\n",
    "        if m:\n",
    "            s.l_la = '1.' + m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2bc95-c626-4455-8e53-d97ac25bcea4",
   "metadata": {
    "id": "77d2bc95-c626-4455-8e53-d97ac25bcea4"
   },
   "outputs": [],
   "source": [
    "# adjust loci for perseus editions\n",
    "\n",
    "errata_file = os.path.join('data', 'changed_loci.txt')\n",
    "errata = pd.read_csv(errata_file, sep='\\t', dtype=str)\n",
    "errata = dict([\n",
    "    (f'{row.author} {row.work} {row.l_fi_old}-{row.l_la_old}', (row.l_fi_new, row.l_la_new))\n",
    "    for row in errata.itertuples()])\n",
    "\n",
    "for s in all_speeches:\n",
    "    key = f'{s.author.name} {s.work.title} {s.l_range}'\n",
    "    if key in errata:\n",
    "        print(f'Corrected {s}', end=' ')\n",
    "        s.l_fi, s.l_la = errata[key]\n",
    "        print(f'to {s}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac54dd8",
   "metadata": {
    "id": "1ac54dd8"
   },
   "source": [
    "## Get the text\n",
    "\n",
    "Because we're retrieving the texts from a local repository I've turned off caching to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7d6bd",
   "metadata": {
    "id": "12e7d6bd"
   },
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for i, s in enumerate(all_speeches):\n",
    "    if (i % 200 == 0) or (i == len(all_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(all_speeches))} % complete', end='')\n",
    "    if not hasattr(s, 'passage') or s.passage is None:\n",
    "        try:\n",
    "            s.passage = cts.getPassage(s, cache=False)\n",
    "        except:\n",
    "            s.passage = None\n",
    "    if s.passage is None:\n",
    "        failed.append(s)\n",
    "\n",
    "print()\n",
    "if DEBUG:\n",
    "    print (f'{len(failed)} failed:')\n",
    "    for s in failed:\n",
    "        print(f'\\t{s.author.name} {s.work.title} {s.l_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad415e9e",
   "metadata": {
    "id": "ad415e9e"
   },
   "source": [
    "### Add supplementary text for speeches not in Perseus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f40fc8",
   "metadata": {
    "id": "18f40fc8"
   },
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'supp_mother_speeches.txt')\n",
    "\n",
    "with open(path) as f:\n",
    "    supplement = json.load(f)\n",
    "\n",
    "for rec in supplement:\n",
    "    for s in all_speeches:\n",
    "        if s.id == rec['id']:\n",
    "            if DEBUG:\n",
    "                print(s)\n",
    "            s.passage = dicesapi.text.Passage()\n",
    "            s.passage.line_array = rec['line_array']\n",
    "            s.passage._line_index = []\n",
    "            cumsum = 0\n",
    "            for i in range(len(s.passage.line_array)):\n",
    "                s.passage._line_index.append(cumsum)\n",
    "                cumsum += len(s.passage.line_array[i]['text']) + 1\n",
    "            s.passage.text = ' '.join([l['text'] for l in s.passage.line_array])\n",
    "            s.passage.speech = s\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2318e",
   "metadata": {
    "id": "49e2318e"
   },
   "source": [
    "### Remove speeches with no text available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96ec23",
   "metadata": {
    "id": "3c96ec23"
   },
   "outputs": [],
   "source": [
    "test_speeches = all_speeches.advancedFilter(lambda s: s.passage is not None).sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b1e15-6e7e-4546-918a-e714ed420b34",
   "metadata": {
    "id": "8b4b1e15-6e7e-4546-918a-e714ed420b34"
   },
   "source": [
    "### Add book number to line array for multi-book speeches\n",
    "\n",
    "We have to add book identifiers to the line numbers in `line_array` for any speech spanning multiple books, in order to make sure that each line has a unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb99aa2-ae10-4e6c-8173-ee7b46d60c9b",
   "metadata": {
    "id": "deb99aa2-ae10-4e6c-8173-ee7b46d60c9b"
   },
   "outputs": [],
   "source": [
    "flagged = []\n",
    "for s in test_speeches:\n",
    "    if '.' not in s.l_fi:\n",
    "        for rec in s.passage.line_array:\n",
    "            rec['N'] = rec['n']\n",
    "    else:\n",
    "        pref_fi, n_fi = s.l_fi.rsplit('.', 1)\n",
    "        pref_la, n_la = s.l_la.rsplit('.', 1)\n",
    "\n",
    "        if pref_fi == pref_la:\n",
    "            n = int(n_fi) - 1\n",
    "\n",
    "            for rec in s.passage.line_array:\n",
    "                if rec['n'] is None:\n",
    "                    n = n + 1\n",
    "                    rec['N'] = pref_fi + '.' + str(n)\n",
    "                    if s not in flagged:\n",
    "                        flagged.append(s)\n",
    "                elif '.' not in rec['n']:\n",
    "                    rec['N'] = pref_fi + '.' + rec['n']\n",
    "                    n = int(rec['n'].replace('a', ''))\n",
    "                else:\n",
    "                    rec['N'] = rec['n']\n",
    "        else:\n",
    "            pref = int(pref_fi)\n",
    "            old_n = int(n_fi)\n",
    "\n",
    "            for rec in s.passage.line_array:\n",
    "                n = int(rec['n'])\n",
    "                if n < (old_n - 100):\n",
    "                    pref = pref + 1\n",
    "                rec['N'] = f'{pref}.{n}'\n",
    "                old_n = n\n",
    "\n",
    "if DEBUG:\n",
    "    for s in flagged:\n",
    "        print(s)\n",
    "        for rec in s.passage.line_array:\n",
    "            print(f'{rec[\"N\"]}\\t{rec[\"text\"]}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74194b1-3a93-4ff9-b594-4da2c53f36ea",
   "metadata": {
    "id": "c74194b1-3a93-4ff9-b594-4da2c53f36ea"
   },
   "source": [
    "### Create fake URNs for any texts that don't have them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd904512-625e-400c-9930-cd2e1796d9ab",
   "metadata": {
    "id": "fd904512-625e-400c-9930-cd2e1796d9ab"
   },
   "outputs": [],
   "source": [
    "for s in test_speeches:\n",
    "    if s.work.urn is None or s.work.urn == '':\n",
    "        s.work.urn = f'{s.work.id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35fb75",
   "metadata": {
    "id": "3d35fb75"
   },
   "source": [
    "# Run NLP\n",
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bae25",
   "metadata": {
    "id": "494bae25"
   },
   "outputs": [],
   "source": [
    "# initialize spacy models\n",
    "spacy_load(\n",
    "    latin_model = 'la_core_web_lg',\n",
    "    greek_model = 'grc_odycy_joint_trf',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0a5fa",
   "metadata": {
    "id": "65a0a5fa"
   },
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for i, s in enumerate(test_speeches):\n",
    "    if (i % 50 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "    if s.passage.spacy_doc is None:\n",
    "        s.passage.runSpacyPipeline()\n",
    "    if s.passage.spacy_doc is None:\n",
    "        failed.append(s)\n",
    "\n",
    "if len(failed) > 0:\n",
    "    print(f'SpaCy failed for {len(failed)} speeches:')\n",
    "    for s in failed:\n",
    "        print(f' - {s.work.urn}\\t{s.work.title}\\t{s.l_range}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64024ab0",
   "metadata": {
    "id": "64024ab0"
   },
   "source": [
    "### Generate tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbbcbb-ba27-48d1-8095-2f9af98cd320",
   "metadata": {
    "id": "75dbbcbb-ba27-48d1-8095-2f9af98cd320"
   },
   "outputs": [],
   "source": [
    "spacy_tokens = []\n",
    "\n",
    "# extract features\n",
    "for i, s in enumerate(test_speeches):\n",
    "    # progress\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "\n",
    "    # process all tokens in speech\n",
    "    for tok in s.passage.spacy_doc:\n",
    "        line_n = s.passage.line_array[s.passage.getLineIndex(tok)]['N'] if s.passage.getLineIndex(tok) is not None else None\n",
    "        spacy_tokens.append(dict(\n",
    "            speech_id = s.id,\n",
    "            lang = s.lang,\n",
    "            author = s.author.name,\n",
    "            work = s.work.title,\n",
    "            urn = s.work.urn,\n",
    "            l_fi = s.l_fi,\n",
    "            l_la = s.l_la,\n",
    "            nlines = len(s.passage.line_array),\n",
    "            spkr = ','.join([inst.name for inst in s.spkr]),\n",
    "            addr = ','.join([inst.name for inst in s.addr]),\n",
    "            part = s.part,\n",
    "            level = s.level,\n",
    "            line_n = line_n,\n",
    "            line_id = f'{s.work.urn}:{line_n}',\n",
    "            token = tok.text,\n",
    "            tok_id = f'{s.id}:{s.passage.getTextPos(tok)}',\n",
    "            lemma = tok.lemma_,\n",
    "            pos = tok.pos_,\n",
    "            mood = tok.morph.get('Mood'),\n",
    "            tense = tok.morph.get('Tense'),\n",
    "            voice = tok.morph.get('Voice'),\n",
    "            person = tok.morph.get('Person'),\n",
    "            number = tok.morph.get('Number'),\n",
    "            case = tok.morph.get('Case'),\n",
    "            gender = tok.morph.get('Gender'),\n",
    "            verbform = tok.morph.get('VerbForm'),\n",
    "            degree = tok.morph.get('Degree'),\n",
    "            prontype = tok.morph.get('PronType'),\n",
    "        ))\n",
    "\n",
    "# convert to data frame\n",
    "spacy_tokens = pd.DataFrame(spacy_tokens)\n",
    "\n",
    "# simplify list cells\n",
    "cols = ['mood', 'tense', 'voice', 'person', 'number', 'case', 'gender', 'verbform', 'degree', 'prontype']\n",
    "spacy_tokens[cols] = spacy_tokens[cols].map(lambda x: None if len(x) == 0 else ','.join(x))\n",
    "\n",
    "# display\n",
    "display(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e697d-e0a5-487f-91a5-aca10ca71e88",
   "metadata": {
    "id": "970e697d-e0a5-487f-91a5-aca10ca71e88"
   },
   "source": [
    "## CLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836a8e7-a8d2-4c40-91d0-4e18d70637aa",
   "metadata": {
    "id": "6836a8e7-a8d2-4c40-91d0-4e18d70637aa"
   },
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for i, s in enumerate(test_speeches):\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "\n",
    "    if s.passage.cltk_doc is None:\n",
    "        try:\n",
    "            s.passage.runCltkPipeline()\n",
    "        except:\n",
    "            print(s)\n",
    "            print(s.passage.text)\n",
    "    if s.passage.cltk_doc is None:\n",
    "        failed.append(s)\n",
    "\n",
    "if len(failed) > 0:\n",
    "    print(f'CLTK failed for {len(failed)} speeches:')\n",
    "    for s in failed:\n",
    "        print(f' - {s.work.urn}\\t{s.work.title}\\t{s.l_range}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dced2b-be7b-439e-af44-4fb0443b595c",
   "metadata": {
    "id": "29dced2b-be7b-439e-af44-4fb0443b595c"
   },
   "outputs": [],
   "source": [
    "# Helper function to extract CLTK features as strings\n",
    "\n",
    "def getCltkFeature(token, feature, default=None):\n",
    "    '''convert token's feature bundle to a dictionary and perform a get'''\n",
    "    d = dict(zip([str(k) for k in token.features.keys()], token.features.values()))\n",
    "    vlist = d.get(feature)\n",
    "\n",
    "    if vlist is None:\n",
    "        return(default)\n",
    "\n",
    "    return [str(v) for v in vlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62beb68d-37eb-4b6b-bd4c-bf3de454ab64",
   "metadata": {
    "id": "62beb68d-37eb-4b6b-bd4c-bf3de454ab64"
   },
   "outputs": [],
   "source": [
    "cltk_tokens = []\n",
    "\n",
    "# extract features\n",
    "for i, s in enumerate(test_speeches):\n",
    "    # progress\n",
    "    if (i % 200 == 0) or (i == len(test_speeches) - 1):\n",
    "        print(f'\\r{round(i * 100 /len(test_speeches))} % complete', end='')\n",
    "\n",
    "    # process all tokens in speech\n",
    "    for tok in s.passage.cltk_doc:\n",
    "        if s.passage.getLineIndex(tok) is not None:\n",
    "            line_n = s.passage.line_array[s.passage.getLineIndex(tok)]['N']\n",
    "        else:\n",
    "            tok_idx = s.passage.getCltkWordIndex(tok)\n",
    "            if tok_idx == 0:\n",
    "                line_n = s.passage.line_array[0]['N']\n",
    "            elif tok_idx == len(s.passage.cltk_doc.words) - 1:\n",
    "                line_n = s.passage.line_array[-1]['N']\n",
    "            else:\n",
    "                left_tok = s.passage.cltk_doc[tok_idx-1]\n",
    "                left_line_idx = s.passage.getLineIndex(left_tok)\n",
    "                right_tok = s.passage.cltk_doc[tok_idx+1]\n",
    "                right_line_idx = s.passage.getLineIndex(right_tok)\n",
    "                if (left_line_idx is not None) and (right_line_idx is not None) and (left_line_idx == right_line_idx):\n",
    "                    line_n = s.passage.line_array[left_line_idx]['N']\n",
    "                else:\n",
    "                    line_n = None\n",
    "        cltk_tokens.append(dict(\n",
    "            speech_id = s.id,\n",
    "            lang = s.lang,\n",
    "            author = s.author.name,\n",
    "            work = s.work.title,\n",
    "            urn = s.work.urn,\n",
    "            l_fi = s.l_fi,\n",
    "            l_la = s.l_la,\n",
    "            nlines = len(s.passage.line_array),\n",
    "            spkr = ','.join([inst.name for inst in s.spkr]),\n",
    "            addr = ','.join([inst.name for inst in s.addr]),\n",
    "            part = s.part,\n",
    "            level = s.level,\n",
    "            line_n = line_n,\n",
    "            line_id = f'{s.work.urn}:{line_n}' if line_n is not None else None,\n",
    "            token = tok.string,\n",
    "            tok_id = f'{s.id}:{s.passage.getTextPos(tok)}',\n",
    "            lemma = tok.lemma,\n",
    "            pos = tok.upos,\n",
    "            mood = getCltkFeature(tok, 'Mood'),\n",
    "            tense = getCltkFeature(tok, 'Tense'),\n",
    "            voice = getCltkFeature(tok, 'Voice'),\n",
    "            aspect = getCltkFeature(tok, 'Aspect'),\n",
    "            person = getCltkFeature(tok, 'Person'),\n",
    "            number = getCltkFeature(tok, 'Number'),\n",
    "            case = getCltkFeature(tok, 'Case'),\n",
    "            gender = getCltkFeature(tok, 'Gender'),\n",
    "            degree = getCltkFeature(tok, 'Degree'),\n",
    "            verbform = getCltkFeature(tok, 'VerbForm'),\n",
    "        ))\n",
    "\n",
    "cltk_tokens = pd.DataFrame(cltk_tokens)\n",
    "\n",
    "# simplify list cells\n",
    "cols = ['mood', 'tense', 'voice', 'aspect', 'person', 'number', 'case', 'gender', 'degree', 'verbform']\n",
    "cltk_tokens[cols] = cltk_tokens[cols].map(lambda x: None if x is None else ','.join(x))\n",
    "\n",
    "# display results\n",
    "display(cltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98834b1e-863c-4290-9341-b5969894da89",
   "metadata": {},
   "source": [
    "## Add Greek question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302cedc-b810-4f85-bd4e-f3e789c111dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_rows = []\n",
    "\n",
    "for s in test_speeches:\n",
    "    if s.lang == \"latin\":\n",
    "        continue\n",
    "    for match in re.finditer(\"(;)\", s.passage.text):\n",
    "        l_idx = 0\n",
    "        for next_l_idx, next_c_idx in enumerate(s.passage._line_index):\n",
    "            if next_c_idx > match.start():\n",
    "                break\n",
    "            else:\n",
    "                l_idx = next_l_idx\n",
    "        line_n = s.passage.line_array[l_idx][\"N\"]        \n",
    "        \n",
    "        extra_rows.append(dict(\n",
    "            speech_id = s.id,\n",
    "            lang = s.lang,\n",
    "            author = s.author.name,\n",
    "            work = s.work.title,\n",
    "            urn = s.work.urn,\n",
    "            l_fi = s.l_fi,\n",
    "            l_la = s.l_la,\n",
    "            nlines = len(s.passage.line_array),\n",
    "            spkr = ','.join([inst.name for inst in s.spkr]),\n",
    "            addr = ','.join([inst.name for inst in s.addr]),\n",
    "            part = s.part,\n",
    "            level = s.level,\n",
    "            line_n = line_n,\n",
    "            line_id = f'{s.work.urn}:{line_n}',\n",
    "            token = \";\",\n",
    "            tok_id = f'{s.id}:{match.start()}',\n",
    "            lemma = \";\",\n",
    "            pos = \"PUNCT\",\n",
    "        ))\n",
    "\n",
    "extra_rows = pd.DataFrame(extra_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ad06c-822a-40f6-93a8-71e0c2c13742",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = (pd.concat([spacy_tokens, extra_rows], ignore_index=True)\n",
    "    .assign(temp=lambda df: df[\"tok_id\"].str.split(\":\"))\n",
    "    .assign(left=lambda df: df[\"temp\"].str[0].astype(int),\n",
    "            right=lambda df: df[\"temp\"].str[1].astype(int))\n",
    "    .sort_values(by=[\"left\", \"right\"])\n",
    "    .drop(columns=[\"left\", \"right\", \"temp\"])\n",
    "    .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760370da-51c9-4ad5-9fef-20f331cff3ea",
   "metadata": {
    "id": "760370da-51c9-4ad5-9fef-20f331cff3ea"
   },
   "source": [
    "## Deduplicate embedded lines\n",
    "\n",
    "Replace any NA values in the **line_id** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14410cb5-a024-4fea-96fc-64177ce851ae",
   "metadata": {
    "id": "14410cb5-a024-4fea-96fc-64177ce851ae"
   },
   "outputs": [],
   "source": [
    "mask = cltk_tokens['line_id'].isna()\n",
    "cltk_tokens.loc[mask, 'line_id'] = cltk_tokens.loc[mask, 'urn'] + ':' + cltk_tokens.loc[mask, 'token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade7514-212c-44a0-a79f-313098eb83b7",
   "metadata": {
    "id": "0ade7514-212c-44a0-a79f-313098eb83b7"
   },
   "outputs": [],
   "source": [
    "max_levels = cltk_tokens.groupby('line_id').agg(level=('level', 'max'))\n",
    "x = cltk_tokens.loc[:,['line_id','level']].merge(max_levels, how='left', on='line_id')\n",
    "mask = x['level_x'] == x['level_y']\n",
    "cltk_no_dups = cltk_tokens.loc[mask]\n",
    "cltk_no_dups.to_csv('cltk_tokens.csv', index=False)\n",
    "display(cltk_no_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b10f1d-420b-457b-b576-d77c7edd3610",
   "metadata": {
    "id": "59b10f1d-420b-457b-b576-d77c7edd3610"
   },
   "outputs": [],
   "source": [
    "x = spacy_tokens.loc[:,['line_id','level']].merge(max_levels, how='left', on='line_id')\n",
    "mask = x['level_x'] == x['level_y']\n",
    "spacy_no_dups = spacy_tokens.loc[mask]\n",
    "spacy_no_dups.to_csv('spacy_tokens.csv', index=False)\n",
    "display(spacy_no_dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77600075-de12-4013-bbb8-100cc2dfa1a7",
   "metadata": {
    "editable": true,
    "id": "77600075-de12-4013-bbb8-100cc2dfa1a7",
    "tags": []
   },
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82528e7-cfb9-4985-9927-f8d332e8c39e",
   "metadata": {
    "id": "f82528e7-cfb9-4985-9927-f8d332e8c39e"
   },
   "outputs": [],
   "source": [
    "cols = ['tok_id', 'token', 'lemma', 'pos', 'mood', 'tense', 'voice', 'aspect', 'person', 'number', 'case', 'gender', 'degree', 'verbform']\n",
    "merged = spacy_no_dups.merge(cltk_no_dups[cols], how='left', on='tok_id', suffixes=('_spacy', '_cltk'))\n",
    "merged.to_csv(os.path.join('data', 'merged.csv'), index=False)\n",
    "display(merged)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
